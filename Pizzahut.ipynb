{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import datetime as dt \n",
    "import copy\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('D:\\Data Intense\\Level 3\\Round 2 - Data set.xlsx', sheet_name='Purchasing data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Reconstruct the data \n",
    "df = df.loc[:, ['CustomerID', 'SalesAmount', 'TransactionDate']]\n",
    "###CALCULATE THE TOTAL GMV\n",
    "df['Total GMV'] = df.groupby('CustomerID')['SalesAmount'].transform('sum')\n",
    "#Take out the last order of each customer \n",
    "df['Last Order'] = df.groupby('CustomerID')['TransactionDate'].transform('max')\n",
    "\n",
    "#Transform last order day \n",
    "df['Last Order'] = pd.to_datetime(df['Last Order'])\n",
    "df['Last Order'] = df['Last Order'].dt.strftime('%Y-%m-%d')\n",
    "df['Last Order'] = pd.to_datetime(df['Last Order'])\n",
    "\n",
    "#Transform the transaction date\n",
    "df['TransactionDate'] = pd.to_datetime(df['TransactionDate'])\n",
    "df['TransactionDate'] = df['TransactionDate'].dt.strftime('%Y-%m-%d')\n",
    "df['TransactionDate'] = pd.to_datetime(df['TransactionDate'])\n",
    "\n",
    "#Create the report date\n",
    "df['Report Date'] = '2023-01-10'\n",
    "df['Report Date'] = pd.to_datetime(df['Report Date'])\n",
    "\n",
    "###CALCULATE THE RECENCY \n",
    "df['Recency'] = (df['Report Date'] - df['Last Order']).dt.days\n",
    "\n",
    "###Calculate the frequency \n",
    "df['Frequency'] = df.groupby('CustomerID')['TransactionDate'].rank(method='dense', ascending=True)\n",
    "df['Max Frequency'] = df.groupby('CustomerID')['Frequency'].transform('max')\n",
    "df['Frequency'] = pd.to_numeric(df['Frequency']).astype('int')\n",
    "\n",
    "#Calculating the recency score\n",
    "max_recency = df['Recency'].max()\n",
    "min_recency = df['Recency'].min()\n",
    "condlist_r = [\n",
    "    (df['Recency'] >= min_recency) & (df['Recency'] <= 0.25*max_recency), \n",
    "    (df['Recency'] > 0.25*max_recency) & (df['Recency'] <= 0.5*max_recency), \n",
    "    (df['Recency'] > 0.5*max_recency) & (df['Recency'] <= 0.75*max_recency), \n",
    "    (df['Recency'] > 0.75*max_recency) & (df['Recency'] <= max_recency)\n",
    "]\n",
    "\n",
    "choicelist_r = [4, 3, 2, 1]\n",
    "\n",
    "df['R'] = np.select(condlist_r, choicelist_r)\n",
    "\n",
    "#Calculating the frequency score \n",
    "max_frequency = df['Frequency'].max()\n",
    "min_frequency = df['Frequency'].min()\n",
    "condlist_f = [\n",
    "    (df['Frequency'] >= min_frequency) & (df['Frequency'] <= 0.25*max_frequency), \n",
    "    (df['Frequency'] > 0.25*max_frequency) & (df['Frequency'] <= 0.5*max_frequency), \n",
    "    (df['Frequency'] > 0.5*max_frequency) & (df['Frequency'] <= 0.75*max_frequency), \n",
    "    (df['Frequency'] > 0.75*max_frequency) & (df['Frequency'] <= max_frequency)\n",
    "]\n",
    "choicelist_f = [1, 2, 3, 4]\n",
    "\n",
    "df['F'] = np.select(condlist_f, choicelist_f)\n",
    "\n",
    "#Calculating the monetary score \n",
    "max_monetary = df['Total GMV'].max() \n",
    "min_monetary = df['Total GMV'].min()\n",
    "condlist_m = [\n",
    "    (df['Total GMV'] >= min_monetary) & (df['Total GMV'] <= 0.25*max_monetary), \n",
    "    (df['Total GMV'] > 0.25*max_monetary) & (df['Total GMV'] <= 0.5*max_monetary), \n",
    "    (df['Total GMV'] > 0.5*max_monetary) & (df['Total GMV'] <= 0.75*max_monetary), \n",
    "    (df['Total GMV'] > 0.75*max_monetary) & (df['Total GMV'] <= max_monetary)\n",
    "]\n",
    "choicelist_m = [1, 2, 3, 4]\n",
    "\n",
    "df['M'] = np.select(condlist_m, choicelist_m)\n",
    "\n",
    "df['RFM'] = df['R'].astype('str') + df['F'].astype('str') + df['M'].astype('str')\n",
    "\n",
    "df['RFM'] = pd.to_numeric(df['RFM'])\n",
    "\n",
    "result = copy.deepcopy(df)\n",
    "result = result.sort_values('TransactionDate').drop_duplicates('CustomerID', keep='last')\n",
    "\n",
    "result = result.loc[:, ['CustomerID', 'Total GMV', 'Last Order', 'Frequency', 'RFM']]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "sns.countplot(x='RFM', data=result, palette='viridis')\n",
    "plt.title('RFM Distribution')\n",
    "\n",
    "plt.xlabel('RFM Score')\n",
    "plt.ylabel('Total Customers')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "from torch.utils.data import TensorDataset, DataLoader \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "data = copy.deepcopy(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      " ----\n",
      "Looked at 0/466913 samples\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 99\u001b[0m\n\u001b[0;32m     96\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m loss_fn(train_pred, train_labels\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mlong))\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m#3. Backward propagation on the loss \u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m train_loss\u001b[38;5;241m.\u001b[39mbackward() \n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m#4. Gradient descending \u001b[39;00m\n\u001b[0;32m    102\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep() \n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:433\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    430\u001b[0m     \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_tensor_str\u001b[38;5;241m.\u001b[39m_str(\u001b[38;5;28mself\u001b[39m, tensor_contents\u001b[38;5;241m=\u001b[39mtensor_contents)\n\u001b[1;32m--> 433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(\n\u001b[0;32m    434\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, inputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    435\u001b[0m ):\n\u001b[0;32m    436\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Computes the gradient of current tensor wrt graph leaves.\u001b[39;00m\n\u001b[0;32m    437\u001b[0m \n\u001b[0;32m    438\u001b[0m \u001b[38;5;124;03m    The graph is differentiated using the chain rule. If the tensor is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[38;5;124;03m            used to compute the attr::tensors.\u001b[39;00m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    482\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import torch \n",
    "# from torch import nn \n",
    "# from torch.utils.data import TensorDataset, DataLoader \n",
    "# from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "# data = copy.deepcopy(result)\n",
    "\n",
    "# #Make a scaler \n",
    "# scaler = StandardScaler() \n",
    "\n",
    "# data['DOW'] = data['Last Order'].dt.dayofweek\n",
    "# input_data = data.loc[:, ['Total GMV', 'Frequency', 'DOW']]\n",
    "# input_data = input_data.values\n",
    "# input_data = torch.tensor(input_data, dtype=torch.float32)\n",
    "\n",
    "# scaler.fit(input_data)\n",
    "\n",
    "# dict_label = {}\n",
    "# list_label = data.loc[:, 'RFM']\n",
    "# list_label = list_label.unique()\n",
    "# list_label = list(list_label)\n",
    "# for index, label in enumerate(list_label): \n",
    "#     dict_label[label] = index\n",
    "# dict_label\n",
    "\n",
    "# result_label = {} \n",
    "# for index, label in enumerate(list_label): \n",
    "#     result_label[index] = label \n",
    "\n",
    "# data['RFM Label'] = data['RFM'].map(dict_label)\n",
    "# label_data = data['RFM Label'].values\n",
    "# label_data = torch.tensor(label_data, dtype=torch.float32)\n",
    "\n",
    "# proportion = int(0.8*len(input_data)) \n",
    "# train_input, train_output = input_data[:proportion], label_data[:proportion]\n",
    "# train_input = scaler.transform(train_input)\n",
    "# train_input = torch.tensor(train_input, dtype=torch.float32)\n",
    "# test_input, test_output = input_data[proportion:], label_data[proportion:]\n",
    "\n",
    "# BATCH_SIZE = 64\n",
    "# train_dataset = TensorDataset(train_input, train_output)\n",
    "# train_dataloader = DataLoader(dataset=train_dataset,\n",
    "#                               batch_size=BATCH_SIZE,\n",
    "#                               shuffle=True)\n",
    "\n",
    "# test_dataset = TensorDataset(test_input, test_output)\n",
    "# test_dataloader = DataLoader(dataset=test_dataset,\n",
    "#                              batch_size=BATCH_SIZE,\n",
    "#                              shuffle=False)\n",
    "\n",
    "# #Create the neural network \n",
    "# class pizzamodel(nn.Module): \n",
    "#     def __init__(self, class_weight=None): \n",
    "#         super().__init__()\n",
    "#         self.stacked_layer = nn.Sequential(\n",
    "#             nn.Linear(in_features=3, out_features=64),\n",
    "#             nn.ReLU(), \n",
    "#             nn.Linear(in_features=64, out_features=32), \n",
    "#             nn.ReLU(), \n",
    "#             nn.Linear(in_features=32, out_features=15)\n",
    "#         )\n",
    "#         self.class_weight = class_weight\n",
    "    \n",
    "#     def forward(self, x): \n",
    "#         return self.stacked_layer(x)\n",
    "\n",
    "# class_weights = {\n",
    "#     '111': 5, \n",
    "#     '211': 5, \n",
    "#     '311': 5, \n",
    "#     '411': 5,\n",
    "#     'others': 3\n",
    "# }\n",
    "\n",
    "# #Initialize the model \n",
    "# model = pizzamodel(class_weight=class_weights) \n",
    "# learning_rate = 0.1\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# loss_fn = nn.CrossEntropyLoss() \n",
    "# def acc_fn(y_pred, y_true): \n",
    "#     correct = torch.eq(y_pred, y_true).sum().item()\n",
    "#     acc = (correct / len(y_pred)) * 100\n",
    "#     return acc \n",
    "\n",
    "# #Create the traning and testing model\n",
    "# epoches = 10\n",
    "# for epoch in range(epoches): \n",
    "#     print(f'Epoch {epoch}\\n ----') \n",
    "#     ###TRAINING MODEL\n",
    "#     model.train() \n",
    "#     for batch, (train_features, train_labels) in enumerate(train_dataloader): \n",
    "#         #1. Pass to the forward \n",
    "#         train_pred = model(train_features)\n",
    "\n",
    "#         #2. Calculate the loss \n",
    "#         train_loss = loss_fn(train_pred, train_labels.type(torch.long))\n",
    "\n",
    "#         #3. Backward propagation on the loss \n",
    "#         train_loss.backward() \n",
    "\n",
    "#         #4. Gradient descending \n",
    "#         optimizer.step() \n",
    "\n",
    "#         #5. Zero out the gradient \n",
    "#         optimizer.zero_grad() \n",
    "\n",
    "#         if batch % 5000 == 0: \n",
    "#             print(f'Looked at {batch * len(train_features)}/{len(train_dataloader.dataset)} samples')\n",
    "    \n",
    "#     ###TESTING MODEL \n",
    "#     model.eval()\n",
    "#     with torch.no_grad(): \n",
    "#         for test_features, test_labels in test_dataloader: \n",
    "#             #1. Pass to the forward \n",
    "#             test_pred = model(test_features)\n",
    "\n",
    "#             #2. Calculate the loss and accuracy \n",
    "#             test_loss = loss_fn(test_pred, test_labels.type(torch.long))\n",
    "#             test_acc = acc_fn(y_pred=torch.argmax(torch.softmax(test_pred, dim=1), dim=1),\n",
    "#                               y_true=test_labels)\n",
    "            \n",
    "#     print(f'Train loss {train_loss.item():.4f} | Test loss {test_loss.item():.4f} | Test accuracy {test_acc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score at 111: 129136\n",
      "Total score at 211: 140772\n",
      "Total score at 311: 149595\n",
      "Total score at 411: 164046\n",
      "Total score of these 4 RFM scores 583549\n",
      "Total score RFM: 583642\n"
     ]
    }
   ],
   "source": [
    "len(data)\n",
    "length_111 = len(data.loc[data['RFM'] == 111, :])\n",
    "length_211 = len(data.loc[data['RFM'] == 211, :])\n",
    "length_311 = len(data.loc[data['RFM'] == 311, :])\n",
    "length_411 = len(data.loc[data['RFM'] == 411, :])\n",
    "print(f'Total score at 111: {length_111}') \n",
    "print(f'Total score at 211: {length_211}')\n",
    "print(f'Total score at 311: {length_311}') \n",
    "print(f'Total score at 411: {length_411}')\n",
    "print(f'Total score of these 4 RFM scores {length_111 + length_211 + length_311 + length_411}')\n",
    "print(f'Total score RFM: {len(data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "scaler = StandardScaler() \n",
    "\n",
    "cond = (df['RFM'] == 111) | (df['RFM'] == 211) | (df['RFM'] == 311) | (df['RFM'] == 411)\n",
    "dominant_data = copy.deepcopy(data.loc[cond, :])\n",
    "dominant_input = dominant_data.loc[:, ['DOW', 'Frequency', 'Total GMV']]\n",
    "# dominant_input = torch.tensor(dominant_input, dtype=torch.float32)\n",
    "\n",
    "others_data = copy.deepcopy(data.loc[~cond, :])\n",
    "others_input = others_data.loc[:, ['Total GMV', 'Frequency', 'DOW']]\n",
    "others_output = others_data.loc[:, ['RFM']]\n",
    "ros = RandomOverSampler(sampling_strategy=\"auto\", random_state=42)\n",
    "x_resampled, y_resampled = ros.fit_resample(others_input, others_output)\n",
    "\n",
    "dominant_indices = dominant_input.index \n",
    "np.random.seed(42)\n",
    "sampled_indices = np.random.choice(dominant_indices, size=704, replace=False)\n",
    "sampled_indices = list(sampled_indices)\n",
    "reduced_dominant = dominant_data.loc[sampled_indices]\n",
    "reduced_input = reduced_dominant.loc[:, ['Total GMV', 'Frequency', 'DOW']]\n",
    "reduced_output = reduced_dominant.loc[:, ['RFM']]\n",
    "\n",
    "pre_input_data = np.concatenate((reduced_input.values, x_resampled.values), axis=0)\n",
    "pre_output_data = np.concatenate((y_resampled.values, reduced_output.values), axis=0)\n",
    "\n",
    "test_df = pd.DataFrame()\n",
    "test_df[['Total GMV', 'Frequency', 'DOW']] = pre_input_data\n",
    "test_df['RFM'] = pre_output_data\n",
    "\n",
    "index_411 = test_df.loc[test_df['RFM'] == 411, :].index\n",
    "np.random.seed(42)\n",
    "drop_411 = len(index_411) - 64\n",
    "indices_411_to_drop = np.random.choice(index_411, size=drop_411, replace=False)\n",
    "test_df.drop(indices_411_to_drop, inplace=True)\n",
    "\n",
    "index_312 = test_df.loc[test_df['RFM'] == 311, :].index\n",
    "np.random.seed(42)\n",
    "drop_312 = len(index_312) - 64\n",
    "indices_312_to_drop = np.random.choice(index_312, size=drop_312, replace=False)\n",
    "test_df.drop(indices_312_to_drop, inplace=True)\n",
    "\n",
    "index_211 = test_df.loc[test_df['RFM'] == 211, :].index\n",
    "np.random.seed(42)\n",
    "drop_211 = len(index_211) - 64\n",
    "indices_211_to_drop = np.random.choice(index_211, size=drop_211, replace=False)\n",
    "test_df.drop(indices_211_to_drop, inplace=True)\n",
    "\n",
    "index_111 = test_df.loc[test_df['RFM'] == 111, :].index\n",
    "np.random.seed(42)\n",
    "drop_111 = len(index_111) - 64\n",
    "indices_111_to_drop = np.random.choice(index_111, size=drop_111, replace=False)\n",
    "test_df.drop(indices_111_to_drop, inplace=True)\n",
    "\n",
    "matching_dict = {}\n",
    "unique_label = list(np.unique(pre_output_data))\n",
    "for index, values in enumerate(unique_label):\n",
    "    matching_dict[values] = index \n",
    "\n",
    "test_df['RFM Label'] = test_df['RFM'].map(matching_dict)\n",
    "\n",
    "input_data = test_df.loc[:, ['Total GMV', 'Frequency', 'DOW']].values\n",
    "output_data = test_df.loc[:, 'RFM Label'].values\n",
    "\n",
    "scaler.fit(input_data)\n",
    "\n",
    "proportion = int(0.8*len(input_data)) \n",
    "train_input, train_label = input_data[:proportion], output_data[:proportion]\n",
    "train_input = scaler.transform(train_input) \n",
    "test_input, test_label = input_data[proportion:], output_data[proportion:]\n",
    "\n",
    "train_input, train_label = torch.tensor(train_input, dtype=torch.float32).clone().detach(), torch.tensor(train_label, dtype=torch.float32).clone().detach()\n",
    "test_input, test_label = torch.tensor(test_input, dtype=torch.float32).clone().detach(), torch.tensor(test_label, dtype=torch.float32).clone().detach()\n",
    "\n",
    "train_dataset = TensorDataset(train_input, train_label)\n",
    "\n",
    "BATCH_SIZE = 32 \n",
    "train_dataloader = DataLoader(dataset=train_dataset,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(test_input, test_label)\n",
    "test_dataloader = DataLoader(dataset=test_dataset,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             shuffle=False)\n",
    "\n",
    "class pizzamodel(nn.Module): \n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "        self.stacked_layer = nn.Sequential(\n",
    "            nn.Linear(in_features=3, out_features=256),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(in_features=256, out_features=256), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(in_features=256, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=15)\n",
    "        )\n",
    "\n",
    "    def forward(self, x): \n",
    "        return self.stacked_layer(x)\n",
    "    \n",
    "sns.countplot(x='RFM', data=test_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.6985 | Test loss 1223720.0000 | Test accuracy 0.0\n",
      "Epoch 1\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.6941 | Test loss 1417735.5000 | Test accuracy 0.0\n",
      "Epoch 2\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.6791 | Test loss 1601372.5000 | Test accuracy 0.0\n",
      "Epoch 3\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.6600 | Test loss 1783205.2500 | Test accuracy 0.0\n",
      "Epoch 4\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.6740 | Test loss 1965824.1250 | Test accuracy 0.0\n",
      "Epoch 5\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.6716 | Test loss 2153087.5000 | Test accuracy 0.0\n",
      "Epoch 6\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.6417 | Test loss 2341371.5000 | Test accuracy 0.0\n",
      "Epoch 7\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.6393 | Test loss 2582358.5000 | Test accuracy 0.0\n",
      "Epoch 8\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.6435 | Test loss 2829494.5000 | Test accuracy 0.0\n",
      "Epoch 9\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.6535 | Test loss 3069503.5000 | Test accuracy 0.0\n",
      "Epoch 10\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.5961 | Test loss 3299092.7500 | Test accuracy 0.0\n",
      "Epoch 11\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.6718 | Test loss 3514367.2500 | Test accuracy 0.0\n",
      "Epoch 12\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.6343 | Test loss 3707406.5000 | Test accuracy 0.0\n",
      "Epoch 13\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.5785 | Test loss 3876776.2500 | Test accuracy 0.0\n",
      "Epoch 14\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.6213 | Test loss 4014550.2500 | Test accuracy 0.0\n",
      "Epoch 15\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.5724 | Test loss 4103034.7500 | Test accuracy 0.0\n",
      "Epoch 16\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.5550 | Test loss 4130727.5000 | Test accuracy 0.0\n",
      "Epoch 17\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.5903 | Test loss 4097866.7500 | Test accuracy 0.0\n",
      "Epoch 18\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.5950 | Test loss 4000708.0000 | Test accuracy 0.0\n",
      "Epoch 19\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.5408 | Test loss 3827327.7500 | Test accuracy 0.0\n",
      "Epoch 20\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.5967 | Test loss 3601084.5000 | Test accuracy 0.0\n",
      "Epoch 21\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.5271 | Test loss 3323660.7500 | Test accuracy 0.0\n",
      "Epoch 22\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.5315 | Test loss 3010539.5000 | Test accuracy 0.0\n",
      "Epoch 23\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.5838 | Test loss 2681233.7500 | Test accuracy 0.0\n",
      "Epoch 24\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.5651 | Test loss 2308347.7500 | Test accuracy 0.0\n",
      "Epoch 25\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.5102 | Test loss 1909867.7500 | Test accuracy 0.0\n",
      "Epoch 26\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.5256 | Test loss 1510448.3750 | Test accuracy 0.0\n",
      "Epoch 27\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.5243 | Test loss 1327390.3750 | Test accuracy 31.25\n",
      "Epoch 28\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.5791 | Test loss 1333183.2500 | Test accuracy 31.25\n",
      "Epoch 29\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4878 | Test loss 1343073.5000 | Test accuracy 31.25\n",
      "Epoch 30\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.5632 | Test loss 1353930.6250 | Test accuracy 31.25\n",
      "Epoch 31\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4816 | Test loss 1358890.2500 | Test accuracy 31.25\n",
      "Epoch 32\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.5382 | Test loss 1366352.3750 | Test accuracy 31.25\n",
      "Epoch 33\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4611 | Test loss 1371884.0000 | Test accuracy 31.25\n",
      "Epoch 34\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.5015 | Test loss 1381669.1250 | Test accuracy 31.25\n",
      "Epoch 35\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4472 | Test loss 1392125.5000 | Test accuracy 31.25\n",
      "Epoch 36\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4845 | Test loss 1408430.8750 | Test accuracy 31.25\n",
      "Epoch 37\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4551 | Test loss 1423144.2500 | Test accuracy 31.25\n",
      "Epoch 38\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4702 | Test loss 1439068.6250 | Test accuracy 31.25\n",
      "Epoch 39\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4723 | Test loss 1460627.3750 | Test accuracy 31.25\n",
      "Epoch 40\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.5447 | Test loss 1485993.2500 | Test accuracy 31.25\n",
      "Epoch 41\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4917 | Test loss 1505383.5000 | Test accuracy 31.25\n",
      "Epoch 42\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4497 | Test loss 1522990.0000 | Test accuracy 31.25\n",
      "Epoch 43\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4050 | Test loss 1541281.3750 | Test accuracy 31.25\n",
      "Epoch 44\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4865 | Test loss 1561315.3750 | Test accuracy 31.25\n",
      "Epoch 45\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4710 | Test loss 1580543.5000 | Test accuracy 31.25\n",
      "Epoch 46\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4560 | Test loss 1595806.7500 | Test accuracy 31.25\n",
      "Epoch 47\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4384 | Test loss 1612622.5000 | Test accuracy 31.25\n",
      "Epoch 48\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3726 | Test loss 1630926.5000 | Test accuracy 31.25\n",
      "Epoch 49\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4437 | Test loss 1645116.0000 | Test accuracy 31.25\n",
      "Epoch 50\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3826 | Test loss 1657935.7500 | Test accuracy 31.25\n",
      "Epoch 51\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4468 | Test loss 1669965.0000 | Test accuracy 31.25\n",
      "Epoch 52\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4005 | Test loss 1686394.6250 | Test accuracy 31.25\n",
      "Epoch 53\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4593 | Test loss 1702243.0000 | Test accuracy 31.25\n",
      "Epoch 54\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3969 | Test loss 1719593.7500 | Test accuracy 31.25\n",
      "Epoch 55\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4221 | Test loss 1737794.8750 | Test accuracy 31.25\n",
      "Epoch 56\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3633 | Test loss 1757658.6250 | Test accuracy 31.25\n",
      "Epoch 57\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4363 | Test loss 1781522.6250 | Test accuracy 31.25\n",
      "Epoch 58\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4357 | Test loss 1805866.0000 | Test accuracy 31.25\n",
      "Epoch 59\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4207 | Test loss 1826793.1250 | Test accuracy 31.25\n",
      "Epoch 60\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3541 | Test loss 1850255.1250 | Test accuracy 31.25\n",
      "Epoch 61\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3654 | Test loss 1873099.0000 | Test accuracy 31.25\n",
      "Epoch 62\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4133 | Test loss 1896566.3750 | Test accuracy 31.25\n",
      "Epoch 63\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3525 | Test loss 1915727.2500 | Test accuracy 31.25\n",
      "Epoch 64\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3628 | Test loss 1935111.1250 | Test accuracy 31.25\n",
      "Epoch 65\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3940 | Test loss 1951490.8750 | Test accuracy 31.25\n",
      "Epoch 66\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3700 | Test loss 1955534.0000 | Test accuracy 31.25\n",
      "Epoch 67\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3450 | Test loss 1959609.0000 | Test accuracy 31.25\n",
      "Epoch 68\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3729 | Test loss 1950867.6250 | Test accuracy 31.25\n",
      "Epoch 69\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3207 | Test loss 1935248.1250 | Test accuracy 31.25\n",
      "Epoch 70\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3775 | Test loss 1908340.0000 | Test accuracy 31.25\n",
      "Epoch 71\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3634 | Test loss 1866134.2500 | Test accuracy 31.25\n",
      "Epoch 72\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3408 | Test loss 1831777.7500 | Test accuracy 31.25\n",
      "Epoch 73\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4455 | Test loss 1763594.7500 | Test accuracy 31.25\n",
      "Epoch 74\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4368 | Test loss 1686177.2500 | Test accuracy 31.25\n",
      "Epoch 75\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3612 | Test loss 1634193.0000 | Test accuracy 31.25\n",
      "Epoch 76\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3209 | Test loss 1567149.5000 | Test accuracy 31.25\n",
      "Epoch 77\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3732 | Test loss 1517382.7500 | Test accuracy 31.25\n",
      "Epoch 78\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3583 | Test loss 1420074.2500 | Test accuracy 31.25\n",
      "Epoch 79\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2883 | Test loss 1307472.0000 | Test accuracy 31.25\n",
      "Epoch 80\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3459 | Test loss 1215663.3750 | Test accuracy 31.25\n",
      "Epoch 81\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2767 | Test loss 1121702.1250 | Test accuracy 31.25\n",
      "Epoch 82\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2948 | Test loss 1042900.0000 | Test accuracy 31.25\n",
      "Epoch 83\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3937 | Test loss 951781.7500 | Test accuracy 31.25\n",
      "Epoch 84\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3261 | Test loss 888269.2500 | Test accuracy 31.25\n",
      "Epoch 85\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3558 | Test loss 777211.8750 | Test accuracy 31.25\n",
      "Epoch 86\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2823 | Test loss 701302.7500 | Test accuracy 31.25\n",
      "Epoch 87\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2864 | Test loss 605326.2500 | Test accuracy 31.25\n",
      "Epoch 88\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3646 | Test loss 544979.5000 | Test accuracy 28.125\n",
      "Epoch 89\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3696 | Test loss 496710.0000 | Test accuracy 28.125\n",
      "Epoch 90\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2602 | Test loss 445587.6250 | Test accuracy 28.125\n",
      "Epoch 91\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3350 | Test loss 483141.3750 | Test accuracy 28.125\n",
      "Epoch 92\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3399 | Test loss 426202.5000 | Test accuracy 28.125\n",
      "Epoch 93\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3969 | Test loss 384123.8750 | Test accuracy 28.125\n",
      "Epoch 94\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2854 | Test loss 357237.0000 | Test accuracy 28.125\n",
      "Epoch 95\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3370 | Test loss 372368.7500 | Test accuracy 28.125\n",
      "Epoch 96\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2664 | Test loss 375306.1250 | Test accuracy 25.0\n",
      "Epoch 97\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2861 | Test loss 352885.2500 | Test accuracy 25.0\n",
      "Epoch 98\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3147 | Test loss 364877.6250 | Test accuracy 25.0\n",
      "Epoch 99\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3382 | Test loss 448996.0000 | Test accuracy 25.0\n",
      "Epoch 100\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2745 | Test loss 505157.0000 | Test accuracy 25.0\n",
      "Epoch 101\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2928 | Test loss 588471.1250 | Test accuracy 25.0\n",
      "Epoch 102\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3739 | Test loss 691115.0000 | Test accuracy 25.0\n",
      "Epoch 103\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3171 | Test loss 615181.3750 | Test accuracy 25.0\n",
      "Epoch 104\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2729 | Test loss 667969.5000 | Test accuracy 25.0\n",
      "Epoch 105\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2944 | Test loss 729503.2500 | Test accuracy 25.0\n",
      "Epoch 106\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1962 | Test loss 836539.0000 | Test accuracy 25.0\n",
      "Epoch 107\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3344 | Test loss 828021.5000 | Test accuracy 25.0\n",
      "Epoch 108\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2233 | Test loss 716563.8750 | Test accuracy 25.0\n",
      "Epoch 109\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2757 | Test loss 732234.0000 | Test accuracy 25.0\n",
      "Epoch 110\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2345 | Test loss 703214.2500 | Test accuracy 25.0\n",
      "Epoch 111\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3484 | Test loss 625191.7500 | Test accuracy 25.0\n",
      "Epoch 112\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2878 | Test loss 695414.2500 | Test accuracy 25.0\n",
      "Epoch 113\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3092 | Test loss 618728.0000 | Test accuracy 25.0\n",
      "Epoch 114\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3225 | Test loss 741489.5000 | Test accuracy 25.0\n",
      "Epoch 115\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3043 | Test loss 712659.2500 | Test accuracy 25.0\n",
      "Epoch 116\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2547 | Test loss 631800.5000 | Test accuracy 25.0\n",
      "Epoch 117\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4080 | Test loss 571412.2500 | Test accuracy 25.0\n",
      "Epoch 118\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3206 | Test loss 521723.2500 | Test accuracy 25.0\n",
      "Epoch 119\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2557 | Test loss 599701.0000 | Test accuracy 25.0\n",
      "Epoch 120\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3299 | Test loss 662382.5000 | Test accuracy 25.0\n",
      "Epoch 121\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1858 | Test loss 593193.7500 | Test accuracy 25.0\n",
      "Epoch 122\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3232 | Test loss 653475.7500 | Test accuracy 25.0\n",
      "Epoch 123\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3405 | Test loss 765004.7500 | Test accuracy 25.0\n",
      "Epoch 124\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2601 | Test loss 598707.7500 | Test accuracy 25.0\n",
      "Epoch 125\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3331 | Test loss 725379.2500 | Test accuracy 25.0\n",
      "Epoch 126\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3700 | Test loss 690660.0000 | Test accuracy 25.0\n",
      "Epoch 127\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3755 | Test loss 663675.2500 | Test accuracy 25.0\n",
      "Epoch 128\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2855 | Test loss 649926.5000 | Test accuracy 25.0\n",
      "Epoch 129\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2612 | Test loss 750103.0000 | Test accuracy 25.0\n",
      "Epoch 130\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3215 | Test loss 685404.0000 | Test accuracy 25.0\n",
      "Epoch 131\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4550 | Test loss 605438.7500 | Test accuracy 31.25\n",
      "Epoch 132\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3992 | Test loss 562206.5000 | Test accuracy 31.25\n",
      "Epoch 133\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3920 | Test loss 803068.2500 | Test accuracy 25.0\n",
      "Epoch 134\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2701 | Test loss 676260.2500 | Test accuracy 25.0\n",
      "Epoch 135\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3950 | Test loss 665426.5000 | Test accuracy 31.25\n",
      "Epoch 136\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3527 | Test loss 652356.2500 | Test accuracy 31.25\n",
      "Epoch 137\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2951 | Test loss 741533.7500 | Test accuracy 31.25\n",
      "Epoch 138\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3137 | Test loss 796770.0000 | Test accuracy 31.25\n",
      "Epoch 139\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3087 | Test loss 813821.5000 | Test accuracy 31.25\n",
      "Epoch 140\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1751 | Test loss 739245.5000 | Test accuracy 25.0\n",
      "Epoch 141\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3063 | Test loss 695176.7500 | Test accuracy 31.25\n",
      "Epoch 142\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4052 | Test loss 668458.7500 | Test accuracy 31.25\n",
      "Epoch 143\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2238 | Test loss 805713.0000 | Test accuracy 25.0\n",
      "Epoch 144\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2857 | Test loss 840286.5000 | Test accuracy 25.0\n",
      "Epoch 145\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3409 | Test loss 1018114.2500 | Test accuracy 25.0\n",
      "Epoch 146\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2791 | Test loss 775231.0000 | Test accuracy 25.0\n",
      "Epoch 147\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3896 | Test loss 708585.5000 | Test accuracy 25.0\n",
      "Epoch 148\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1698 | Test loss 848894.0000 | Test accuracy 25.0\n",
      "Epoch 149\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3060 | Test loss 877986.2500 | Test accuracy 25.0\n",
      "Epoch 150\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3446 | Test loss 890705.0000 | Test accuracy 25.0\n",
      "Epoch 151\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3447 | Test loss 1057528.2500 | Test accuracy 25.0\n",
      "Epoch 152\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3572 | Test loss 1186365.2500 | Test accuracy 25.0\n",
      "Epoch 153\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2754 | Test loss 1014669.5000 | Test accuracy 25.0\n",
      "Epoch 154\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3297 | Test loss 954989.7500 | Test accuracy 25.0\n",
      "Epoch 155\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2252 | Test loss 1029130.5000 | Test accuracy 25.0\n",
      "Epoch 156\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2584 | Test loss 997579.0000 | Test accuracy 25.0\n",
      "Epoch 157\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3310 | Test loss 893521.7500 | Test accuracy 25.0\n",
      "Epoch 158\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2334 | Test loss 853918.0000 | Test accuracy 25.0\n",
      "Epoch 159\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2383 | Test loss 796710.7500 | Test accuracy 31.25\n",
      "Epoch 160\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2197 | Test loss 779720.5000 | Test accuracy 31.25\n",
      "Epoch 161\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3540 | Test loss 845501.7500 | Test accuracy 31.25\n",
      "Epoch 162\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3079 | Test loss 871736.5000 | Test accuracy 25.0\n",
      "Epoch 163\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4027 | Test loss 913270.0000 | Test accuracy 31.25\n",
      "Epoch 164\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2742 | Test loss 859515.5000 | Test accuracy 31.25\n",
      "Epoch 165\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2111 | Test loss 840039.5000 | Test accuracy 25.0\n",
      "Epoch 166\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2019 | Test loss 926511.7500 | Test accuracy 25.0\n",
      "Epoch 167\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3781 | Test loss 977989.5000 | Test accuracy 31.25\n",
      "Epoch 168\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3370 | Test loss 780200.2500 | Test accuracy 31.25\n",
      "Epoch 169\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2908 | Test loss 835486.7500 | Test accuracy 31.25\n",
      "Epoch 170\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3396 | Test loss 888070.0000 | Test accuracy 31.25\n",
      "Epoch 171\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2492 | Test loss 924483.2500 | Test accuracy 25.0\n",
      "Epoch 172\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2659 | Test loss 1039233.2500 | Test accuracy 25.0\n",
      "Epoch 173\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3356 | Test loss 1178122.5000 | Test accuracy 31.25\n",
      "Epoch 174\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2749 | Test loss 1079058.5000 | Test accuracy 31.25\n",
      "Epoch 175\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3580 | Test loss 1202857.7500 | Test accuracy 31.25\n",
      "Epoch 176\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2891 | Test loss 1277866.2500 | Test accuracy 31.25\n",
      "Epoch 177\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2444 | Test loss 935881.0000 | Test accuracy 25.0\n",
      "Epoch 178\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3664 | Test loss 982805.5000 | Test accuracy 25.0\n",
      "Epoch 179\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2987 | Test loss 960583.5000 | Test accuracy 25.0\n",
      "Epoch 180\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2843 | Test loss 1152788.5000 | Test accuracy 31.25\n",
      "Epoch 181\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3412 | Test loss 807320.0000 | Test accuracy 31.25\n",
      "Epoch 182\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3141 | Test loss 1027650.7500 | Test accuracy 25.0\n",
      "Epoch 183\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3107 | Test loss 1290907.5000 | Test accuracy 25.0\n",
      "Epoch 184\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2034 | Test loss 1333062.7500 | Test accuracy 25.0\n",
      "Epoch 185\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3013 | Test loss 1236087.7500 | Test accuracy 25.0\n",
      "Epoch 186\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2811 | Test loss 1146887.7500 | Test accuracy 31.25\n",
      "Epoch 187\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2817 | Test loss 1036240.2500 | Test accuracy 31.25\n",
      "Epoch 188\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3161 | Test loss 1144143.7500 | Test accuracy 31.25\n",
      "Epoch 189\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2336 | Test loss 1172070.2500 | Test accuracy 31.25\n",
      "Epoch 190\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3858 | Test loss 887619.7500 | Test accuracy 25.0\n",
      "Epoch 191\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2996 | Test loss 1006558.7500 | Test accuracy 31.25\n",
      "Epoch 192\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1886 | Test loss 1240338.0000 | Test accuracy 25.0\n",
      "Epoch 193\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3492 | Test loss 1093027.2500 | Test accuracy 31.25\n",
      "Epoch 194\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2157 | Test loss 1420859.7500 | Test accuracy 31.25\n",
      "Epoch 195\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3175 | Test loss 1172790.7500 | Test accuracy 31.25\n",
      "Epoch 196\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4418 | Test loss 1084785.5000 | Test accuracy 25.0\n",
      "Epoch 197\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2264 | Test loss 1171153.7500 | Test accuracy 31.25\n",
      "Epoch 198\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3270 | Test loss 973179.0000 | Test accuracy 25.0\n",
      "Epoch 199\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3434 | Test loss 1501335.0000 | Test accuracy 31.25\n",
      "Epoch 200\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4004 | Test loss 1739736.5000 | Test accuracy 31.25\n",
      "Epoch 201\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3109 | Test loss 1378048.5000 | Test accuracy 31.25\n",
      "Epoch 202\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3380 | Test loss 1522605.7500 | Test accuracy 31.25\n",
      "Epoch 203\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3526 | Test loss 1497641.0000 | Test accuracy 31.25\n",
      "Epoch 204\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2760 | Test loss 1388373.0000 | Test accuracy 31.25\n",
      "Epoch 205\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2086 | Test loss 1622625.2500 | Test accuracy 31.25\n",
      "Epoch 206\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4002 | Test loss 1808770.5000 | Test accuracy 31.25\n",
      "Epoch 207\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2706 | Test loss 1465485.5000 | Test accuracy 31.25\n",
      "Epoch 208\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2981 | Test loss 1729955.7500 | Test accuracy 31.25\n",
      "Epoch 209\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3431 | Test loss 1784268.5000 | Test accuracy 31.25\n",
      "Epoch 210\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2873 | Test loss 1458997.2500 | Test accuracy 31.25\n",
      "Epoch 211\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3533 | Test loss 1473617.5000 | Test accuracy 31.25\n",
      "Epoch 212\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3140 | Test loss 1562290.5000 | Test accuracy 31.25\n",
      "Epoch 213\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3972 | Test loss 1544417.2500 | Test accuracy 31.25\n",
      "Epoch 214\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2534 | Test loss 1525798.5000 | Test accuracy 31.25\n",
      "Epoch 215\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2245 | Test loss 1380572.5000 | Test accuracy 25.0\n",
      "Epoch 216\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2337 | Test loss 1524199.7500 | Test accuracy 31.25\n",
      "Epoch 217\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3380 | Test loss 1351124.0000 | Test accuracy 31.25\n",
      "Epoch 218\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2943 | Test loss 1236181.5000 | Test accuracy 25.0\n",
      "Epoch 219\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3657 | Test loss 1283559.2500 | Test accuracy 31.25\n",
      "Epoch 220\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2191 | Test loss 1448759.2500 | Test accuracy 31.25\n",
      "Epoch 221\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2719 | Test loss 1689691.5000 | Test accuracy 31.25\n",
      "Epoch 222\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3287 | Test loss 1547738.0000 | Test accuracy 31.25\n",
      "Epoch 223\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3943 | Test loss 1444266.5000 | Test accuracy 31.25\n",
      "Epoch 224\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2741 | Test loss 1348117.7500 | Test accuracy 31.25\n",
      "Epoch 225\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3180 | Test loss 1513869.7500 | Test accuracy 31.25\n",
      "Epoch 226\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2241 | Test loss 1499153.7500 | Test accuracy 31.25\n",
      "Epoch 227\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3716 | Test loss 1430486.7500 | Test accuracy 25.0\n",
      "Epoch 228\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3608 | Test loss 1640237.5000 | Test accuracy 25.0\n",
      "Epoch 229\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2140 | Test loss 1731046.7500 | Test accuracy 31.25\n",
      "Epoch 230\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2424 | Test loss 1621805.2500 | Test accuracy 31.25\n",
      "Epoch 231\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2862 | Test loss 1371142.7500 | Test accuracy 31.25\n",
      "Epoch 232\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2850 | Test loss 1688943.7500 | Test accuracy 31.25\n",
      "Epoch 233\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3951 | Test loss 1652067.2500 | Test accuracy 31.25\n",
      "Epoch 234\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3386 | Test loss 1479755.7500 | Test accuracy 31.25\n",
      "Epoch 235\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2539 | Test loss 1958101.0000 | Test accuracy 31.25\n",
      "Epoch 236\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2637 | Test loss 1647927.7500 | Test accuracy 25.0\n",
      "Epoch 237\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1641 | Test loss 1733994.7500 | Test accuracy 31.25\n",
      "Epoch 238\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2202 | Test loss 1644012.2500 | Test accuracy 31.25\n",
      "Epoch 239\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2945 | Test loss 1969346.2500 | Test accuracy 31.25\n",
      "Epoch 240\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2529 | Test loss 1991920.7500 | Test accuracy 31.25\n",
      "Epoch 241\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2343 | Test loss 1904835.2500 | Test accuracy 31.25\n",
      "Epoch 242\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3073 | Test loss 1978536.5000 | Test accuracy 31.25\n",
      "Epoch 243\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3015 | Test loss 2019714.7500 | Test accuracy 31.25\n",
      "Epoch 244\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2465 | Test loss 1690657.2500 | Test accuracy 31.25\n",
      "Epoch 245\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2213 | Test loss 1774029.7500 | Test accuracy 31.25\n",
      "Epoch 246\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3460 | Test loss 1764331.0000 | Test accuracy 31.25\n",
      "Epoch 247\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2667 | Test loss 1929845.2500 | Test accuracy 31.25\n",
      "Epoch 248\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1839 | Test loss 1714706.2500 | Test accuracy 31.25\n",
      "Epoch 249\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1767 | Test loss 1900022.7500 | Test accuracy 31.25\n",
      "Epoch 250\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3491 | Test loss 1794637.7500 | Test accuracy 31.25\n",
      "Epoch 251\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2596 | Test loss 1943331.2500 | Test accuracy 31.25\n",
      "Epoch 252\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4058 | Test loss 2280482.0000 | Test accuracy 31.25\n",
      "Epoch 253\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2640 | Test loss 2278447.0000 | Test accuracy 31.25\n",
      "Epoch 254\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2669 | Test loss 2428124.7500 | Test accuracy 31.25\n",
      "Epoch 255\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3582 | Test loss 2368689.5000 | Test accuracy 31.25\n",
      "Epoch 256\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2389 | Test loss 2197912.7500 | Test accuracy 31.25\n",
      "Epoch 257\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4170 | Test loss 1995886.0000 | Test accuracy 31.25\n",
      "Epoch 258\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3113 | Test loss 1787453.2500 | Test accuracy 31.25\n",
      "Epoch 259\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2189 | Test loss 1592456.5000 | Test accuracy 31.25\n",
      "Epoch 260\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3569 | Test loss 1873098.2500 | Test accuracy 31.25\n",
      "Epoch 261\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2566 | Test loss 2029202.2500 | Test accuracy 31.25\n",
      "Epoch 262\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2382 | Test loss 2161678.5000 | Test accuracy 31.25\n",
      "Epoch 263\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4102 | Test loss 2149318.0000 | Test accuracy 31.25\n",
      "Epoch 264\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2984 | Test loss 2143015.7500 | Test accuracy 31.25\n",
      "Epoch 265\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2029 | Test loss 1861811.7500 | Test accuracy 31.25\n",
      "Epoch 266\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3558 | Test loss 1827408.2500 | Test accuracy 31.25\n",
      "Epoch 267\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3490 | Test loss 2006057.0000 | Test accuracy 31.25\n",
      "Epoch 268\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3580 | Test loss 2303185.2500 | Test accuracy 31.25\n",
      "Epoch 269\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2755 | Test loss 2321387.7500 | Test accuracy 31.25\n",
      "Epoch 270\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3042 | Test loss 2043865.2500 | Test accuracy 31.25\n",
      "Epoch 271\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2430 | Test loss 1718164.7500 | Test accuracy 31.25\n",
      "Epoch 272\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3959 | Test loss 1892501.0000 | Test accuracy 31.25\n",
      "Epoch 273\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2590 | Test loss 2156443.0000 | Test accuracy 31.25\n",
      "Epoch 274\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3646 | Test loss 2004842.0000 | Test accuracy 31.25\n",
      "Epoch 275\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1275 | Test loss 2243831.7500 | Test accuracy 31.25\n",
      "Epoch 276\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2162 | Test loss 2223013.0000 | Test accuracy 31.25\n",
      "Epoch 277\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2105 | Test loss 2103162.2500 | Test accuracy 31.25\n",
      "Epoch 278\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3624 | Test loss 1750955.5000 | Test accuracy 31.25\n",
      "Epoch 279\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2628 | Test loss 1829285.2500 | Test accuracy 31.25\n",
      "Epoch 280\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3302 | Test loss 2346859.7500 | Test accuracy 31.25\n",
      "Epoch 281\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2466 | Test loss 2243808.7500 | Test accuracy 31.25\n",
      "Epoch 282\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2925 | Test loss 2675083.7500 | Test accuracy 31.25\n",
      "Epoch 283\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3014 | Test loss 2308438.2500 | Test accuracy 31.25\n",
      "Epoch 284\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2553 | Test loss 2401746.7500 | Test accuracy 31.25\n",
      "Epoch 285\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3209 | Test loss 2226156.0000 | Test accuracy 31.25\n",
      "Epoch 286\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3566 | Test loss 2646906.2500 | Test accuracy 31.25\n",
      "Epoch 287\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2630 | Test loss 2882689.5000 | Test accuracy 31.25\n",
      "Epoch 288\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3483 | Test loss 2392940.0000 | Test accuracy 31.25\n",
      "Epoch 289\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2304 | Test loss 2425645.5000 | Test accuracy 31.25\n",
      "Epoch 290\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3610 | Test loss 2643224.5000 | Test accuracy 31.25\n",
      "Epoch 291\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2939 | Test loss 2733606.7500 | Test accuracy 31.25\n",
      "Epoch 292\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3647 | Test loss 2849293.2500 | Test accuracy 31.25\n",
      "Epoch 293\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2498 | Test loss 2467255.2500 | Test accuracy 31.25\n",
      "Epoch 294\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4373 | Test loss 2133549.7500 | Test accuracy 31.25\n",
      "Epoch 295\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3451 | Test loss 2616308.5000 | Test accuracy 31.25\n",
      "Epoch 296\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3256 | Test loss 2578552.5000 | Test accuracy 31.25\n",
      "Epoch 297\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3782 | Test loss 3165528.2500 | Test accuracy 31.25\n",
      "Epoch 298\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3160 | Test loss 2650133.5000 | Test accuracy 31.25\n",
      "Epoch 299\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3074 | Test loss 2759764.2500 | Test accuracy 31.25\n",
      "Epoch 300\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3378 | Test loss 3027554.2500 | Test accuracy 31.25\n",
      "Epoch 301\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3585 | Test loss 3019583.0000 | Test accuracy 31.25\n",
      "Epoch 302\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2728 | Test loss 2859074.5000 | Test accuracy 31.25\n",
      "Epoch 303\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3666 | Test loss 2948276.7500 | Test accuracy 31.25\n",
      "Epoch 304\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3174 | Test loss 2953566.7500 | Test accuracy 31.25\n",
      "Epoch 305\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2985 | Test loss 3069557.0000 | Test accuracy 31.25\n",
      "Epoch 306\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2343 | Test loss 2985442.5000 | Test accuracy 31.25\n",
      "Epoch 307\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2735 | Test loss 2593787.2500 | Test accuracy 31.25\n",
      "Epoch 308\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2482 | Test loss 2627148.5000 | Test accuracy 31.25\n",
      "Epoch 309\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4561 | Test loss 2958672.2500 | Test accuracy 31.25\n",
      "Epoch 310\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3869 | Test loss 2599011.7500 | Test accuracy 31.25\n",
      "Epoch 311\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3039 | Test loss 3286569.5000 | Test accuracy 31.25\n",
      "Epoch 312\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2533 | Test loss 3643411.0000 | Test accuracy 31.25\n",
      "Epoch 313\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3619 | Test loss 3334947.7500 | Test accuracy 31.25\n",
      "Epoch 314\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2928 | Test loss 3391147.2500 | Test accuracy 31.25\n",
      "Epoch 315\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3124 | Test loss 3132165.2500 | Test accuracy 31.25\n",
      "Epoch 316\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2147 | Test loss 3211151.7500 | Test accuracy 31.25\n",
      "Epoch 317\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2283 | Test loss 3170585.7500 | Test accuracy 31.25\n",
      "Epoch 318\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3827 | Test loss 3269229.0000 | Test accuracy 31.25\n",
      "Epoch 319\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1504 | Test loss 3154762.2500 | Test accuracy 31.25\n",
      "Epoch 320\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3396 | Test loss 2915835.7500 | Test accuracy 31.25\n",
      "Epoch 321\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2758 | Test loss 3124340.7500 | Test accuracy 31.25\n",
      "Epoch 322\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3756 | Test loss 3187156.7500 | Test accuracy 31.25\n",
      "Epoch 323\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2895 | Test loss 3843941.0000 | Test accuracy 31.25\n",
      "Epoch 324\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1636 | Test loss 3756763.5000 | Test accuracy 31.25\n",
      "Epoch 325\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2934 | Test loss 3216424.5000 | Test accuracy 31.25\n",
      "Epoch 326\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4049 | Test loss 3031282.0000 | Test accuracy 31.25\n",
      "Epoch 327\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3784 | Test loss 3197968.2500 | Test accuracy 31.25\n",
      "Epoch 328\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2961 | Test loss 3062077.2500 | Test accuracy 31.25\n",
      "Epoch 329\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2584 | Test loss 3343359.0000 | Test accuracy 31.25\n",
      "Epoch 330\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2242 | Test loss 3461560.2500 | Test accuracy 31.25\n",
      "Epoch 331\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3359 | Test loss 3413094.2500 | Test accuracy 31.25\n",
      "Epoch 332\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2122 | Test loss 3189253.2500 | Test accuracy 31.25\n",
      "Epoch 333\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3233 | Test loss 3343431.5000 | Test accuracy 31.25\n",
      "Epoch 334\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3793 | Test loss 3075500.5000 | Test accuracy 31.25\n",
      "Epoch 335\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3273 | Test loss 3395563.0000 | Test accuracy 31.25\n",
      "Epoch 336\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3762 | Test loss 3195353.0000 | Test accuracy 31.25\n",
      "Epoch 337\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2684 | Test loss 3573112.0000 | Test accuracy 31.25\n",
      "Epoch 338\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3267 | Test loss 3443718.7500 | Test accuracy 31.25\n",
      "Epoch 339\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2008 | Test loss 3528799.5000 | Test accuracy 31.25\n",
      "Epoch 340\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1568 | Test loss 3500293.5000 | Test accuracy 31.25\n",
      "Epoch 341\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4080 | Test loss 3517944.0000 | Test accuracy 31.25\n",
      "Epoch 342\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3389 | Test loss 3376180.2500 | Test accuracy 31.25\n",
      "Epoch 343\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3640 | Test loss 3512331.0000 | Test accuracy 31.25\n",
      "Epoch 344\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3537 | Test loss 3338343.2500 | Test accuracy 31.25\n",
      "Epoch 345\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4675 | Test loss 3530817.0000 | Test accuracy 31.25\n",
      "Epoch 346\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2650 | Test loss 3687284.2500 | Test accuracy 31.25\n",
      "Epoch 347\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2410 | Test loss 3803007.2500 | Test accuracy 31.25\n",
      "Epoch 348\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2731 | Test loss 3596378.2500 | Test accuracy 31.25\n",
      "Epoch 349\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2688 | Test loss 4028752.5000 | Test accuracy 31.25\n",
      "Epoch 350\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2808 | Test loss 3528286.7500 | Test accuracy 31.25\n",
      "Epoch 351\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2659 | Test loss 3998628.2500 | Test accuracy 31.25\n",
      "Epoch 352\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4044 | Test loss 4120385.0000 | Test accuracy 31.25\n",
      "Epoch 353\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3582 | Test loss 3580056.2500 | Test accuracy 31.25\n",
      "Epoch 354\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3161 | Test loss 3534659.7500 | Test accuracy 31.25\n",
      "Epoch 355\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2712 | Test loss 3600293.7500 | Test accuracy 31.25\n",
      "Epoch 356\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3146 | Test loss 3570612.5000 | Test accuracy 31.25\n",
      "Epoch 357\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1352 | Test loss 3274471.0000 | Test accuracy 31.25\n",
      "Epoch 358\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2385 | Test loss 3356962.5000 | Test accuracy 31.25\n",
      "Epoch 359\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3142 | Test loss 3428505.0000 | Test accuracy 31.25\n",
      "Epoch 360\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3015 | Test loss 3633246.7500 | Test accuracy 31.25\n",
      "Epoch 361\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2253 | Test loss 3698572.2500 | Test accuracy 31.25\n",
      "Epoch 362\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3595 | Test loss 3977620.0000 | Test accuracy 31.25\n",
      "Epoch 363\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3026 | Test loss 3788201.7500 | Test accuracy 31.25\n",
      "Epoch 364\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3341 | Test loss 4178202.0000 | Test accuracy 31.25\n",
      "Epoch 365\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2984 | Test loss 4359611.0000 | Test accuracy 31.25\n",
      "Epoch 366\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2916 | Test loss 4054151.2500 | Test accuracy 31.25\n",
      "Epoch 367\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3311 | Test loss 3482479.2500 | Test accuracy 31.25\n",
      "Epoch 368\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3500 | Test loss 4170971.7500 | Test accuracy 31.25\n",
      "Epoch 369\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3828 | Test loss 4019621.7500 | Test accuracy 31.25\n",
      "Epoch 370\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3682 | Test loss 3754579.0000 | Test accuracy 31.25\n",
      "Epoch 371\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3319 | Test loss 3879066.5000 | Test accuracy 31.25\n",
      "Epoch 372\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3505 | Test loss 4396235.5000 | Test accuracy 31.25\n",
      "Epoch 373\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3181 | Test loss 4135108.5000 | Test accuracy 31.25\n",
      "Epoch 374\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3879 | Test loss 3829523.0000 | Test accuracy 31.25\n",
      "Epoch 375\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3568 | Test loss 4161847.2500 | Test accuracy 31.25\n",
      "Epoch 376\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1711 | Test loss 3767124.2500 | Test accuracy 31.25\n",
      "Epoch 377\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3864 | Test loss 4062235.7500 | Test accuracy 31.25\n",
      "Epoch 378\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3401 | Test loss 3978682.0000 | Test accuracy 31.25\n",
      "Epoch 379\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3488 | Test loss 4905210.5000 | Test accuracy 31.25\n",
      "Epoch 380\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2058 | Test loss 4812465.0000 | Test accuracy 31.25\n",
      "Epoch 381\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2720 | Test loss 4283208.0000 | Test accuracy 31.25\n",
      "Epoch 382\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2530 | Test loss 4563427.5000 | Test accuracy 31.25\n",
      "Epoch 383\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3196 | Test loss 4447103.5000 | Test accuracy 31.25\n",
      "Epoch 384\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2564 | Test loss 4362069.0000 | Test accuracy 31.25\n",
      "Epoch 385\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2486 | Test loss 4094145.5000 | Test accuracy 31.25\n",
      "Epoch 386\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2623 | Test loss 3880888.0000 | Test accuracy 31.25\n",
      "Epoch 387\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3937 | Test loss 3958141.7500 | Test accuracy 31.25\n",
      "Epoch 388\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3269 | Test loss 4234658.0000 | Test accuracy 31.25\n",
      "Epoch 389\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2801 | Test loss 4427936.0000 | Test accuracy 31.25\n",
      "Epoch 390\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1633 | Test loss 3956426.0000 | Test accuracy 31.25\n",
      "Epoch 391\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1737 | Test loss 4312158.5000 | Test accuracy 31.25\n",
      "Epoch 392\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2751 | Test loss 4350015.0000 | Test accuracy 31.25\n",
      "Epoch 393\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3327 | Test loss 4797372.0000 | Test accuracy 31.25\n",
      "Epoch 394\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2599 | Test loss 4643063.0000 | Test accuracy 31.25\n",
      "Epoch 395\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2848 | Test loss 4692316.5000 | Test accuracy 31.25\n",
      "Epoch 396\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3568 | Test loss 4962519.5000 | Test accuracy 31.25\n",
      "Epoch 397\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3677 | Test loss 4307557.0000 | Test accuracy 31.25\n",
      "Epoch 398\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2872 | Test loss 4669016.5000 | Test accuracy 31.25\n",
      "Epoch 399\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3369 | Test loss 4659774.5000 | Test accuracy 31.25\n",
      "Epoch 400\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3315 | Test loss 4837137.5000 | Test accuracy 31.25\n",
      "Epoch 401\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2832 | Test loss 5189999.0000 | Test accuracy 31.25\n",
      "Epoch 402\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2160 | Test loss 5584826.0000 | Test accuracy 31.25\n",
      "Epoch 403\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1866 | Test loss 5560444.0000 | Test accuracy 31.25\n",
      "Epoch 404\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2446 | Test loss 4839878.0000 | Test accuracy 31.25\n",
      "Epoch 405\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2683 | Test loss 4905978.5000 | Test accuracy 31.25\n",
      "Epoch 406\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3261 | Test loss 4552129.5000 | Test accuracy 31.25\n",
      "Epoch 407\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2914 | Test loss 4372103.0000 | Test accuracy 31.25\n",
      "Epoch 408\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1159 | Test loss 4549911.5000 | Test accuracy 31.25\n",
      "Epoch 409\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2239 | Test loss 4620823.0000 | Test accuracy 31.25\n",
      "Epoch 410\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2769 | Test loss 4732204.0000 | Test accuracy 31.25\n",
      "Epoch 411\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2462 | Test loss 3944679.0000 | Test accuracy 31.25\n",
      "Epoch 412\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2239 | Test loss 4341180.5000 | Test accuracy 31.25\n",
      "Epoch 413\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1527 | Test loss 4402234.5000 | Test accuracy 31.25\n",
      "Epoch 414\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3206 | Test loss 5074307.5000 | Test accuracy 31.25\n",
      "Epoch 415\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3161 | Test loss 4977702.5000 | Test accuracy 31.25\n",
      "Epoch 416\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3820 | Test loss 4819146.5000 | Test accuracy 31.25\n",
      "Epoch 417\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2714 | Test loss 4724066.5000 | Test accuracy 31.25\n",
      "Epoch 418\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2991 | Test loss 5486893.5000 | Test accuracy 31.25\n",
      "Epoch 419\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2975 | Test loss 5252110.5000 | Test accuracy 31.25\n",
      "Epoch 420\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3292 | Test loss 4555622.5000 | Test accuracy 31.25\n",
      "Epoch 421\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2480 | Test loss 4154583.2500 | Test accuracy 31.25\n",
      "Epoch 422\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3037 | Test loss 4400831.0000 | Test accuracy 31.25\n",
      "Epoch 423\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3069 | Test loss 5067899.5000 | Test accuracy 31.25\n",
      "Epoch 424\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3011 | Test loss 4274084.5000 | Test accuracy 31.25\n",
      "Epoch 425\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2973 | Test loss 4572830.0000 | Test accuracy 31.25\n",
      "Epoch 426\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3480 | Test loss 5231545.0000 | Test accuracy 31.25\n",
      "Epoch 427\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3060 | Test loss 5227558.5000 | Test accuracy 31.25\n",
      "Epoch 428\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2804 | Test loss 5428720.5000 | Test accuracy 31.25\n",
      "Epoch 429\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2952 | Test loss 5466664.5000 | Test accuracy 31.25\n",
      "Epoch 430\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2855 | Test loss 4958913.0000 | Test accuracy 31.25\n",
      "Epoch 431\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2915 | Test loss 5272066.0000 | Test accuracy 31.25\n",
      "Epoch 432\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1400 | Test loss 5438809.0000 | Test accuracy 31.25\n",
      "Epoch 433\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2578 | Test loss 5141381.0000 | Test accuracy 31.25\n",
      "Epoch 434\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3438 | Test loss 4852066.0000 | Test accuracy 31.25\n",
      "Epoch 435\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3924 | Test loss 4976476.0000 | Test accuracy 31.25\n",
      "Epoch 436\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2472 | Test loss 4656982.5000 | Test accuracy 31.25\n",
      "Epoch 437\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1550 | Test loss 4816854.5000 | Test accuracy 31.25\n",
      "Epoch 438\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2894 | Test loss 5024026.0000 | Test accuracy 31.25\n",
      "Epoch 439\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1950 | Test loss 5259588.0000 | Test accuracy 31.25\n",
      "Epoch 440\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2348 | Test loss 5492616.5000 | Test accuracy 31.25\n",
      "Epoch 441\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3096 | Test loss 5414280.0000 | Test accuracy 31.25\n",
      "Epoch 442\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1815 | Test loss 5250040.0000 | Test accuracy 31.25\n",
      "Epoch 443\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1898 | Test loss 5571884.0000 | Test accuracy 31.25\n",
      "Epoch 444\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2325 | Test loss 5707583.5000 | Test accuracy 31.25\n",
      "Epoch 445\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2311 | Test loss 5035222.5000 | Test accuracy 31.25\n",
      "Epoch 446\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2777 | Test loss 5056288.0000 | Test accuracy 31.25\n",
      "Epoch 447\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2829 | Test loss 5567647.0000 | Test accuracy 31.25\n",
      "Epoch 448\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2999 | Test loss 5573000.0000 | Test accuracy 31.25\n",
      "Epoch 449\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2986 | Test loss 5428275.5000 | Test accuracy 31.25\n",
      "Epoch 450\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4068 | Test loss 5936696.5000 | Test accuracy 31.25\n",
      "Epoch 451\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3081 | Test loss 6069935.5000 | Test accuracy 31.25\n",
      "Epoch 452\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3752 | Test loss 5785866.0000 | Test accuracy 31.25\n",
      "Epoch 453\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3717 | Test loss 6147872.5000 | Test accuracy 31.25\n",
      "Epoch 454\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3026 | Test loss 5819917.5000 | Test accuracy 31.25\n",
      "Epoch 455\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2389 | Test loss 5854669.0000 | Test accuracy 31.25\n",
      "Epoch 456\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3428 | Test loss 5925023.5000 | Test accuracy 31.25\n",
      "Epoch 457\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3565 | Test loss 4754944.5000 | Test accuracy 31.25\n",
      "Epoch 458\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3261 | Test loss 6225116.0000 | Test accuracy 31.25\n",
      "Epoch 459\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2576 | Test loss 6168721.0000 | Test accuracy 31.25\n",
      "Epoch 460\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3118 | Test loss 6175217.5000 | Test accuracy 31.25\n",
      "Epoch 461\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2571 | Test loss 5327003.0000 | Test accuracy 31.25\n",
      "Epoch 462\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2428 | Test loss 5158048.0000 | Test accuracy 31.25\n",
      "Epoch 463\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3174 | Test loss 5659595.5000 | Test accuracy 31.25\n",
      "Epoch 464\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3061 | Test loss 5657238.0000 | Test accuracy 31.25\n",
      "Epoch 465\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3158 | Test loss 6473760.0000 | Test accuracy 31.25\n",
      "Epoch 466\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4113 | Test loss 6536376.0000 | Test accuracy 31.25\n",
      "Epoch 467\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3117 | Test loss 6322442.5000 | Test accuracy 31.25\n",
      "Epoch 468\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3052 | Test loss 5924657.0000 | Test accuracy 31.25\n",
      "Epoch 469\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2666 | Test loss 5327886.5000 | Test accuracy 31.25\n",
      "Epoch 470\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2227 | Test loss 5363886.0000 | Test accuracy 31.25\n",
      "Epoch 471\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1649 | Test loss 6173942.5000 | Test accuracy 31.25\n",
      "Epoch 472\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1686 | Test loss 5365864.0000 | Test accuracy 31.25\n",
      "Epoch 473\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2186 | Test loss 5467042.5000 | Test accuracy 31.25\n",
      "Epoch 474\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1920 | Test loss 5688292.5000 | Test accuracy 31.25\n",
      "Epoch 475\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3565 | Test loss 6084710.0000 | Test accuracy 31.25\n",
      "Epoch 476\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2995 | Test loss 6027140.0000 | Test accuracy 31.25\n",
      "Epoch 477\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3529 | Test loss 5943449.5000 | Test accuracy 31.25\n",
      "Epoch 478\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3049 | Test loss 6004565.5000 | Test accuracy 31.25\n",
      "Epoch 479\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1977 | Test loss 5933458.0000 | Test accuracy 31.25\n",
      "Epoch 480\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2048 | Test loss 5848615.5000 | Test accuracy 31.25\n",
      "Epoch 481\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2092 | Test loss 6403766.0000 | Test accuracy 31.25\n",
      "Epoch 482\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3020 | Test loss 6711366.5000 | Test accuracy 31.25\n",
      "Epoch 483\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3554 | Test loss 6358493.0000 | Test accuracy 31.25\n",
      "Epoch 484\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3254 | Test loss 6000712.5000 | Test accuracy 31.25\n",
      "Epoch 485\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2854 | Test loss 6469473.0000 | Test accuracy 31.25\n",
      "Epoch 486\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2984 | Test loss 6342625.5000 | Test accuracy 31.25\n",
      "Epoch 487\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3697 | Test loss 6573989.0000 | Test accuracy 31.25\n",
      "Epoch 488\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2915 | Test loss 6538835.5000 | Test accuracy 31.25\n",
      "Epoch 489\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3846 | Test loss 5992298.0000 | Test accuracy 31.25\n",
      "Epoch 490\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1667 | Test loss 6496605.5000 | Test accuracy 31.25\n",
      "Epoch 491\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3859 | Test loss 6504062.5000 | Test accuracy 31.25\n",
      "Epoch 492\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2925 | Test loss 5992369.5000 | Test accuracy 31.25\n",
      "Epoch 493\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3443 | Test loss 5784227.5000 | Test accuracy 31.25\n",
      "Epoch 494\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3194 | Test loss 6260831.5000 | Test accuracy 31.25\n",
      "Epoch 495\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2996 | Test loss 6231239.5000 | Test accuracy 31.25\n",
      "Epoch 496\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2682 | Test loss 6233521.5000 | Test accuracy 31.25\n",
      "Epoch 497\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3294 | Test loss 6698496.5000 | Test accuracy 31.25\n",
      "Epoch 498\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3228 | Test loss 6655674.5000 | Test accuracy 31.25\n",
      "Epoch 499\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4096 | Test loss 6954086.0000 | Test accuracy 31.25\n",
      "Epoch 500\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3676 | Test loss 6116436.0000 | Test accuracy 31.25\n",
      "Epoch 501\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2070 | Test loss 5398551.0000 | Test accuracy 31.25\n",
      "Epoch 502\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2118 | Test loss 5567016.5000 | Test accuracy 31.25\n",
      "Epoch 503\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2465 | Test loss 5557079.0000 | Test accuracy 31.25\n",
      "Epoch 504\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3041 | Test loss 5453724.0000 | Test accuracy 31.25\n",
      "Epoch 505\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2642 | Test loss 5688156.0000 | Test accuracy 31.25\n",
      "Epoch 506\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4035 | Test loss 5235884.0000 | Test accuracy 31.25\n",
      "Epoch 507\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3367 | Test loss 5619884.5000 | Test accuracy 31.25\n",
      "Epoch 508\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3100 | Test loss 5420933.5000 | Test accuracy 31.25\n",
      "Epoch 509\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2920 | Test loss 6051592.5000 | Test accuracy 31.25\n",
      "Epoch 510\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3102 | Test loss 5843608.5000 | Test accuracy 31.25\n",
      "Epoch 511\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1265 | Test loss 5677458.0000 | Test accuracy 31.25\n",
      "Epoch 512\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2459 | Test loss 6517161.0000 | Test accuracy 31.25\n",
      "Epoch 513\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2322 | Test loss 6341450.0000 | Test accuracy 31.25\n",
      "Epoch 514\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3180 | Test loss 5640486.0000 | Test accuracy 31.25\n",
      "Epoch 515\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3864 | Test loss 6630298.0000 | Test accuracy 31.25\n",
      "Epoch 516\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2980 | Test loss 5984817.5000 | Test accuracy 31.25\n",
      "Epoch 517\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2611 | Test loss 6354527.0000 | Test accuracy 31.25\n",
      "Epoch 518\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3404 | Test loss 6287017.5000 | Test accuracy 31.25\n",
      "Epoch 519\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2826 | Test loss 6122648.5000 | Test accuracy 31.25\n",
      "Epoch 520\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2748 | Test loss 6184823.5000 | Test accuracy 31.25\n",
      "Epoch 521\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2250 | Test loss 6626663.5000 | Test accuracy 31.25\n",
      "Epoch 522\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2170 | Test loss 6187669.5000 | Test accuracy 31.25\n",
      "Epoch 523\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3951 | Test loss 5323638.5000 | Test accuracy 31.25\n",
      "Epoch 524\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3232 | Test loss 5879388.0000 | Test accuracy 31.25\n",
      "Epoch 525\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3729 | Test loss 6007357.5000 | Test accuracy 31.25\n",
      "Epoch 526\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2751 | Test loss 6407890.0000 | Test accuracy 31.25\n",
      "Epoch 527\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2440 | Test loss 6419811.5000 | Test accuracy 31.25\n",
      "Epoch 528\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3187 | Test loss 6867738.0000 | Test accuracy 31.25\n",
      "Epoch 529\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3106 | Test loss 7448896.0000 | Test accuracy 31.25\n",
      "Epoch 530\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2919 | Test loss 7240081.0000 | Test accuracy 31.25\n",
      "Epoch 531\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2503 | Test loss 6639795.0000 | Test accuracy 31.25\n",
      "Epoch 532\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2863 | Test loss 6172830.5000 | Test accuracy 31.25\n",
      "Epoch 533\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2673 | Test loss 6468809.0000 | Test accuracy 31.25\n",
      "Epoch 534\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1603 | Test loss 7221523.0000 | Test accuracy 31.25\n",
      "Epoch 535\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2438 | Test loss 7577171.5000 | Test accuracy 31.25\n",
      "Epoch 536\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2776 | Test loss 6660742.0000 | Test accuracy 31.25\n",
      "Epoch 537\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3443 | Test loss 6273300.5000 | Test accuracy 31.25\n",
      "Epoch 538\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1536 | Test loss 7304239.5000 | Test accuracy 31.25\n",
      "Epoch 539\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3987 | Test loss 6564888.5000 | Test accuracy 31.25\n",
      "Epoch 540\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3802 | Test loss 6544276.5000 | Test accuracy 31.25\n",
      "Epoch 541\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4221 | Test loss 7504376.0000 | Test accuracy 31.25\n",
      "Epoch 542\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3472 | Test loss 6953200.5000 | Test accuracy 31.25\n",
      "Epoch 543\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2629 | Test loss 7379711.5000 | Test accuracy 31.25\n",
      "Epoch 544\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2367 | Test loss 6744583.0000 | Test accuracy 31.25\n",
      "Epoch 545\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3486 | Test loss 6833557.5000 | Test accuracy 31.25\n",
      "Epoch 546\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3527 | Test loss 6934082.5000 | Test accuracy 31.25\n",
      "Epoch 547\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3494 | Test loss 6257923.0000 | Test accuracy 31.25\n",
      "Epoch 548\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3372 | Test loss 7296567.5000 | Test accuracy 31.25\n",
      "Epoch 549\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2136 | Test loss 7817853.5000 | Test accuracy 31.25\n",
      "Epoch 550\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1736 | Test loss 6782962.0000 | Test accuracy 31.25\n",
      "Epoch 551\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2053 | Test loss 7210988.5000 | Test accuracy 31.25\n",
      "Epoch 552\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3024 | Test loss 6082303.5000 | Test accuracy 31.25\n",
      "Epoch 553\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3540 | Test loss 6446892.5000 | Test accuracy 31.25\n",
      "Epoch 554\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1880 | Test loss 6746329.0000 | Test accuracy 31.25\n",
      "Epoch 555\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2414 | Test loss 7979737.5000 | Test accuracy 31.25\n",
      "Epoch 556\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3059 | Test loss 7335148.5000 | Test accuracy 31.25\n",
      "Epoch 557\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1562 | Test loss 7342475.5000 | Test accuracy 31.25\n",
      "Epoch 558\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3152 | Test loss 6933353.0000 | Test accuracy 31.25\n",
      "Epoch 559\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2805 | Test loss 7960594.5000 | Test accuracy 31.25\n",
      "Epoch 560\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2821 | Test loss 7667498.0000 | Test accuracy 31.25\n",
      "Epoch 561\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2841 | Test loss 7621343.5000 | Test accuracy 31.25\n",
      "Epoch 562\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2511 | Test loss 7600236.0000 | Test accuracy 31.25\n",
      "Epoch 563\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3473 | Test loss 6437935.0000 | Test accuracy 31.25\n",
      "Epoch 564\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2511 | Test loss 6628874.0000 | Test accuracy 31.25\n",
      "Epoch 565\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3247 | Test loss 6888772.0000 | Test accuracy 31.25\n",
      "Epoch 566\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3176 | Test loss 7039215.5000 | Test accuracy 31.25\n",
      "Epoch 567\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3710 | Test loss 7055199.0000 | Test accuracy 31.25\n",
      "Epoch 568\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2955 | Test loss 6833996.0000 | Test accuracy 31.25\n",
      "Epoch 569\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3213 | Test loss 7339953.5000 | Test accuracy 31.25\n",
      "Epoch 570\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3874 | Test loss 7717815.5000 | Test accuracy 31.25\n",
      "Epoch 571\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3117 | Test loss 7838358.0000 | Test accuracy 31.25\n",
      "Epoch 572\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2473 | Test loss 7905310.5000 | Test accuracy 31.25\n",
      "Epoch 573\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3438 | Test loss 7855415.5000 | Test accuracy 31.25\n",
      "Epoch 574\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3357 | Test loss 7333914.5000 | Test accuracy 31.25\n",
      "Epoch 575\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1822 | Test loss 7957750.0000 | Test accuracy 31.25\n",
      "Epoch 576\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1057 | Test loss 7545770.5000 | Test accuracy 31.25\n",
      "Epoch 577\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1811 | Test loss 8163371.5000 | Test accuracy 31.25\n",
      "Epoch 578\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3154 | Test loss 7755219.5000 | Test accuracy 31.25\n",
      "Epoch 579\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3111 | Test loss 7845805.0000 | Test accuracy 31.25\n",
      "Epoch 580\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3527 | Test loss 7815676.5000 | Test accuracy 31.25\n",
      "Epoch 581\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3002 | Test loss 7610408.0000 | Test accuracy 31.25\n",
      "Epoch 582\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2091 | Test loss 6884014.0000 | Test accuracy 31.25\n",
      "Epoch 583\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2695 | Test loss 7136366.5000 | Test accuracy 31.25\n",
      "Epoch 584\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3795 | Test loss 6491572.5000 | Test accuracy 31.25\n",
      "Epoch 585\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2653 | Test loss 7801980.5000 | Test accuracy 31.25\n",
      "Epoch 586\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1767 | Test loss 7075450.0000 | Test accuracy 31.25\n",
      "Epoch 587\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2233 | Test loss 7938112.0000 | Test accuracy 31.25\n",
      "Epoch 588\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2623 | Test loss 7996657.5000 | Test accuracy 31.25\n",
      "Epoch 589\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3692 | Test loss 7457146.5000 | Test accuracy 31.25\n",
      "Epoch 590\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3176 | Test loss 7520344.0000 | Test accuracy 31.25\n",
      "Epoch 591\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2835 | Test loss 6441019.5000 | Test accuracy 31.25\n",
      "Epoch 592\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3417 | Test loss 6713193.0000 | Test accuracy 31.25\n",
      "Epoch 593\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2146 | Test loss 7910952.5000 | Test accuracy 31.25\n",
      "Epoch 594\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.0903 | Test loss 7170957.0000 | Test accuracy 31.25\n",
      "Epoch 595\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3024 | Test loss 7360620.0000 | Test accuracy 31.25\n",
      "Epoch 596\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.0142 | Test loss 8126717.5000 | Test accuracy 31.25\n",
      "Epoch 597\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1932 | Test loss 7947831.5000 | Test accuracy 31.25\n",
      "Epoch 598\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3926 | Test loss 7942475.0000 | Test accuracy 31.25\n",
      "Epoch 599\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2135 | Test loss 8161532.0000 | Test accuracy 31.25\n",
      "Epoch 600\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2715 | Test loss 7015656.0000 | Test accuracy 31.25\n",
      "Epoch 601\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2994 | Test loss 6936234.5000 | Test accuracy 31.25\n",
      "Epoch 602\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3281 | Test loss 7881151.5000 | Test accuracy 31.25\n",
      "Epoch 603\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2310 | Test loss 7527117.0000 | Test accuracy 31.25\n",
      "Epoch 604\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2797 | Test loss 7119702.0000 | Test accuracy 31.25\n",
      "Epoch 605\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2533 | Test loss 6884211.5000 | Test accuracy 31.25\n",
      "Epoch 606\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2960 | Test loss 7433753.0000 | Test accuracy 31.25\n",
      "Epoch 607\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3125 | Test loss 7454051.0000 | Test accuracy 31.25\n",
      "Epoch 608\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3556 | Test loss 8106249.5000 | Test accuracy 31.25\n",
      "Epoch 609\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2379 | Test loss 7600147.5000 | Test accuracy 31.25\n",
      "Epoch 610\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2317 | Test loss 8026784.5000 | Test accuracy 31.25\n",
      "Epoch 611\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2556 | Test loss 8220092.5000 | Test accuracy 31.25\n",
      "Epoch 612\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2346 | Test loss 7826529.0000 | Test accuracy 31.25\n",
      "Epoch 613\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2932 | Test loss 8719316.0000 | Test accuracy 31.25\n",
      "Epoch 614\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2410 | Test loss 7672033.5000 | Test accuracy 31.25\n",
      "Epoch 615\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2542 | Test loss 7984568.5000 | Test accuracy 31.25\n",
      "Epoch 616\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3438 | Test loss 7649798.0000 | Test accuracy 31.25\n",
      "Epoch 617\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2482 | Test loss 7695901.0000 | Test accuracy 31.25\n",
      "Epoch 618\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2346 | Test loss 7980136.5000 | Test accuracy 31.25\n",
      "Epoch 619\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3652 | Test loss 8279380.5000 | Test accuracy 31.25\n",
      "Epoch 620\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1407 | Test loss 8301619.0000 | Test accuracy 31.25\n",
      "Epoch 621\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2104 | Test loss 7670204.5000 | Test accuracy 31.25\n",
      "Epoch 622\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3184 | Test loss 8781506.0000 | Test accuracy 31.25\n",
      "Epoch 623\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2817 | Test loss 9165454.0000 | Test accuracy 31.25\n",
      "Epoch 624\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4198 | Test loss 9195102.0000 | Test accuracy 31.25\n",
      "Epoch 625\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2496 | Test loss 8014703.0000 | Test accuracy 31.25\n",
      "Epoch 626\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2653 | Test loss 8044599.0000 | Test accuracy 31.25\n",
      "Epoch 627\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.0866 | Test loss 8644676.0000 | Test accuracy 31.25\n",
      "Epoch 628\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3427 | Test loss 8219635.5000 | Test accuracy 31.25\n",
      "Epoch 629\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3647 | Test loss 8507648.0000 | Test accuracy 31.25\n",
      "Epoch 630\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2511 | Test loss 8577726.0000 | Test accuracy 31.25\n",
      "Epoch 631\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2954 | Test loss 8643722.0000 | Test accuracy 31.25\n",
      "Epoch 632\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3301 | Test loss 8580282.0000 | Test accuracy 31.25\n",
      "Epoch 633\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2902 | Test loss 8285175.0000 | Test accuracy 31.25\n",
      "Epoch 634\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2836 | Test loss 7659881.5000 | Test accuracy 31.25\n",
      "Epoch 635\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1867 | Test loss 7923050.0000 | Test accuracy 31.25\n",
      "Epoch 636\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3042 | Test loss 8262266.5000 | Test accuracy 31.25\n",
      "Epoch 637\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2706 | Test loss 8172191.0000 | Test accuracy 31.25\n",
      "Epoch 638\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2797 | Test loss 7651065.5000 | Test accuracy 31.25\n",
      "Epoch 639\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1132 | Test loss 7872717.0000 | Test accuracy 31.25\n",
      "Epoch 640\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3102 | Test loss 8986464.0000 | Test accuracy 31.25\n",
      "Epoch 641\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2480 | Test loss 8838888.0000 | Test accuracy 31.25\n",
      "Epoch 642\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2387 | Test loss 8829964.0000 | Test accuracy 31.25\n",
      "Epoch 643\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2727 | Test loss 8076863.5000 | Test accuracy 31.25\n",
      "Epoch 644\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2199 | Test loss 8984318.0000 | Test accuracy 31.25\n",
      "Epoch 645\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3180 | Test loss 8416284.0000 | Test accuracy 31.25\n",
      "Epoch 646\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2734 | Test loss 8883214.0000 | Test accuracy 31.25\n",
      "Epoch 647\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1791 | Test loss 8662634.0000 | Test accuracy 31.25\n",
      "Epoch 648\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2949 | Test loss 8625574.0000 | Test accuracy 31.25\n",
      "Epoch 649\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3169 | Test loss 8471767.0000 | Test accuracy 31.25\n",
      "Epoch 650\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.0998 | Test loss 8873324.0000 | Test accuracy 31.25\n",
      "Epoch 651\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4095 | Test loss 8899842.0000 | Test accuracy 31.25\n",
      "Epoch 652\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3434 | Test loss 8463916.0000 | Test accuracy 31.25\n",
      "Epoch 653\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3279 | Test loss 8643979.0000 | Test accuracy 31.25\n",
      "Epoch 654\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2617 | Test loss 8010759.5000 | Test accuracy 31.25\n",
      "Epoch 655\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4197 | Test loss 8003708.5000 | Test accuracy 31.25\n",
      "Epoch 656\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2762 | Test loss 8282499.5000 | Test accuracy 31.25\n",
      "Epoch 657\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1716 | Test loss 8860582.0000 | Test accuracy 31.25\n",
      "Epoch 658\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3957 | Test loss 8564989.0000 | Test accuracy 31.25\n",
      "Epoch 659\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2725 | Test loss 7818295.5000 | Test accuracy 31.25\n",
      "Epoch 660\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2893 | Test loss 8756146.0000 | Test accuracy 31.25\n",
      "Epoch 661\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2120 | Test loss 9046707.0000 | Test accuracy 31.25\n",
      "Epoch 662\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2756 | Test loss 8238945.0000 | Test accuracy 31.25\n",
      "Epoch 663\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3399 | Test loss 8569327.0000 | Test accuracy 31.25\n",
      "Epoch 664\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2896 | Test loss 9133759.0000 | Test accuracy 31.25\n",
      "Epoch 665\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4028 | Test loss 8572449.0000 | Test accuracy 31.25\n",
      "Epoch 666\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3582 | Test loss 8526348.0000 | Test accuracy 31.25\n",
      "Epoch 667\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1510 | Test loss 8395292.0000 | Test accuracy 31.25\n",
      "Epoch 668\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2887 | Test loss 9244190.0000 | Test accuracy 31.25\n",
      "Epoch 669\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2561 | Test loss 9902969.0000 | Test accuracy 31.25\n",
      "Epoch 670\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2928 | Test loss 9290638.0000 | Test accuracy 31.25\n",
      "Epoch 671\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2772 | Test loss 9018816.0000 | Test accuracy 31.25\n",
      "Epoch 672\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3766 | Test loss 8915349.0000 | Test accuracy 31.25\n",
      "Epoch 673\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2634 | Test loss 9203832.0000 | Test accuracy 31.25\n",
      "Epoch 674\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2407 | Test loss 10182567.0000 | Test accuracy 31.25\n",
      "Epoch 675\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3186 | Test loss 8814919.0000 | Test accuracy 31.25\n",
      "Epoch 676\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1457 | Test loss 9590246.0000 | Test accuracy 31.25\n",
      "Epoch 677\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2477 | Test loss 8303462.0000 | Test accuracy 31.25\n",
      "Epoch 678\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2269 | Test loss 8395610.0000 | Test accuracy 31.25\n",
      "Epoch 679\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3011 | Test loss 8000846.5000 | Test accuracy 31.25\n",
      "Epoch 680\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1994 | Test loss 8634338.0000 | Test accuracy 31.25\n",
      "Epoch 681\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2398 | Test loss 8665400.0000 | Test accuracy 31.25\n",
      "Epoch 682\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3163 | Test loss 9256880.0000 | Test accuracy 31.25\n",
      "Epoch 683\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2041 | Test loss 10149126.0000 | Test accuracy 31.25\n",
      "Epoch 684\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.0906 | Test loss 8326099.5000 | Test accuracy 31.25\n",
      "Epoch 685\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2431 | Test loss 8641280.0000 | Test accuracy 31.25\n",
      "Epoch 686\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2718 | Test loss 9156829.0000 | Test accuracy 31.25\n",
      "Epoch 687\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2170 | Test loss 8400608.0000 | Test accuracy 31.25\n",
      "Epoch 688\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1693 | Test loss 9260643.0000 | Test accuracy 31.25\n",
      "Epoch 689\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1818 | Test loss 8887787.0000 | Test accuracy 31.25\n",
      "Epoch 690\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2323 | Test loss 9005157.0000 | Test accuracy 31.25\n",
      "Epoch 691\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1792 | Test loss 9462360.0000 | Test accuracy 31.25\n",
      "Epoch 692\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2535 | Test loss 9087666.0000 | Test accuracy 31.25\n",
      "Epoch 693\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3033 | Test loss 8848890.0000 | Test accuracy 31.25\n",
      "Epoch 694\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2877 | Test loss 9214563.0000 | Test accuracy 31.25\n",
      "Epoch 695\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3098 | Test loss 8739982.0000 | Test accuracy 31.25\n",
      "Epoch 696\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1198 | Test loss 9057014.0000 | Test accuracy 31.25\n",
      "Epoch 697\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3459 | Test loss 8521010.0000 | Test accuracy 31.25\n",
      "Epoch 698\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3780 | Test loss 8951150.0000 | Test accuracy 31.25\n",
      "Epoch 699\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3067 | Test loss 9410080.0000 | Test accuracy 31.25\n",
      "Epoch 700\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3067 | Test loss 8910854.0000 | Test accuracy 31.25\n",
      "Epoch 701\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2138 | Test loss 7935454.5000 | Test accuracy 31.25\n",
      "Epoch 702\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2537 | Test loss 8375936.0000 | Test accuracy 31.25\n",
      "Epoch 703\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3333 | Test loss 9475478.0000 | Test accuracy 31.25\n",
      "Epoch 704\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1862 | Test loss 9837358.0000 | Test accuracy 31.25\n",
      "Epoch 705\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2396 | Test loss 10030696.0000 | Test accuracy 31.25\n",
      "Epoch 706\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3421 | Test loss 10642742.0000 | Test accuracy 31.25\n",
      "Epoch 707\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3036 | Test loss 10402007.0000 | Test accuracy 31.25\n",
      "Epoch 708\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3653 | Test loss 9779589.0000 | Test accuracy 31.25\n",
      "Epoch 709\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2592 | Test loss 9741446.0000 | Test accuracy 31.25\n",
      "Epoch 710\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2488 | Test loss 9287624.0000 | Test accuracy 31.25\n",
      "Epoch 711\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2769 | Test loss 9266514.0000 | Test accuracy 31.25\n",
      "Epoch 712\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3483 | Test loss 9496436.0000 | Test accuracy 31.25\n",
      "Epoch 713\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2703 | Test loss 11020497.0000 | Test accuracy 31.25\n",
      "Epoch 714\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2695 | Test loss 9856526.0000 | Test accuracy 31.25\n",
      "Epoch 715\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2843 | Test loss 9868626.0000 | Test accuracy 31.25\n",
      "Epoch 716\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3169 | Test loss 9480282.0000 | Test accuracy 31.25\n",
      "Epoch 717\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2733 | Test loss 9443154.0000 | Test accuracy 31.25\n",
      "Epoch 718\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2402 | Test loss 8908159.0000 | Test accuracy 31.25\n",
      "Epoch 719\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3332 | Test loss 10086662.0000 | Test accuracy 31.25\n",
      "Epoch 720\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3981 | Test loss 9558892.0000 | Test accuracy 31.25\n",
      "Epoch 721\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2473 | Test loss 10986306.0000 | Test accuracy 31.25\n",
      "Epoch 722\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4122 | Test loss 9600858.0000 | Test accuracy 31.25\n",
      "Epoch 723\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2164 | Test loss 8563956.0000 | Test accuracy 31.25\n",
      "Epoch 724\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3555 | Test loss 9455074.0000 | Test accuracy 31.25\n",
      "Epoch 725\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3173 | Test loss 9443554.0000 | Test accuracy 31.25\n",
      "Epoch 726\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.0166 | Test loss 9022120.0000 | Test accuracy 31.25\n",
      "Epoch 727\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2566 | Test loss 9489230.0000 | Test accuracy 31.25\n",
      "Epoch 728\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3021 | Test loss 9850639.0000 | Test accuracy 31.25\n",
      "Epoch 729\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2132 | Test loss 10615654.0000 | Test accuracy 31.25\n",
      "Epoch 730\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2245 | Test loss 10016954.0000 | Test accuracy 31.25\n",
      "Epoch 731\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2393 | Test loss 11299354.0000 | Test accuracy 31.25\n",
      "Epoch 732\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2717 | Test loss 10909854.0000 | Test accuracy 31.25\n",
      "Epoch 733\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3276 | Test loss 9934609.0000 | Test accuracy 31.25\n",
      "Epoch 734\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3183 | Test loss 10729134.0000 | Test accuracy 31.25\n",
      "Epoch 735\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2309 | Test loss 9497606.0000 | Test accuracy 31.25\n",
      "Epoch 736\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2835 | Test loss 9418516.0000 | Test accuracy 31.25\n",
      "Epoch 737\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3175 | Test loss 10347614.0000 | Test accuracy 31.25\n",
      "Epoch 738\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3547 | Test loss 10696006.0000 | Test accuracy 31.25\n",
      "Epoch 739\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3561 | Test loss 11141844.0000 | Test accuracy 31.25\n",
      "Epoch 740\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3209 | Test loss 10533931.0000 | Test accuracy 31.25\n",
      "Epoch 741\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1667 | Test loss 11084344.0000 | Test accuracy 31.25\n",
      "Epoch 742\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2823 | Test loss 11150106.0000 | Test accuracy 31.25\n",
      "Epoch 743\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1966 | Test loss 10914540.0000 | Test accuracy 31.25\n",
      "Epoch 744\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4313 | Test loss 11129140.0000 | Test accuracy 31.25\n",
      "Epoch 745\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2558 | Test loss 10414780.0000 | Test accuracy 31.25\n",
      "Epoch 746\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2378 | Test loss 10546572.0000 | Test accuracy 31.25\n",
      "Epoch 747\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2671 | Test loss 10759142.0000 | Test accuracy 31.25\n",
      "Epoch 748\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2328 | Test loss 9753758.0000 | Test accuracy 31.25\n",
      "Epoch 749\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2626 | Test loss 9721358.0000 | Test accuracy 31.25\n",
      "Epoch 750\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2510 | Test loss 9558493.0000 | Test accuracy 31.25\n",
      "Epoch 751\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4943 | Test loss 9477688.0000 | Test accuracy 31.25\n",
      "Epoch 752\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3124 | Test loss 9844560.0000 | Test accuracy 31.25\n",
      "Epoch 753\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2725 | Test loss 9809974.0000 | Test accuracy 31.25\n",
      "Epoch 754\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2273 | Test loss 11189944.0000 | Test accuracy 31.25\n",
      "Epoch 755\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3623 | Test loss 10459611.0000 | Test accuracy 31.25\n",
      "Epoch 756\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2996 | Test loss 10705035.0000 | Test accuracy 31.25\n",
      "Epoch 757\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2281 | Test loss 10074387.0000 | Test accuracy 31.25\n",
      "Epoch 758\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2909 | Test loss 9951828.0000 | Test accuracy 31.25\n",
      "Epoch 759\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3782 | Test loss 9575910.0000 | Test accuracy 31.25\n",
      "Epoch 760\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4063 | Test loss 10249080.0000 | Test accuracy 31.25\n",
      "Epoch 761\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2821 | Test loss 10252316.0000 | Test accuracy 31.25\n",
      "Epoch 762\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3175 | Test loss 10609766.0000 | Test accuracy 31.25\n",
      "Epoch 763\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2001 | Test loss 10951420.0000 | Test accuracy 31.25\n",
      "Epoch 764\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3268 | Test loss 10648370.0000 | Test accuracy 31.25\n",
      "Epoch 765\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.0559 | Test loss 9971966.0000 | Test accuracy 31.25\n",
      "Epoch 766\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1855 | Test loss 9961678.0000 | Test accuracy 31.25\n",
      "Epoch 767\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2834 | Test loss 10113465.0000 | Test accuracy 31.25\n",
      "Epoch 768\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1691 | Test loss 10253600.0000 | Test accuracy 31.25\n",
      "Epoch 769\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3359 | Test loss 10507967.0000 | Test accuracy 31.25\n",
      "Epoch 770\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3987 | Test loss 9870350.0000 | Test accuracy 31.25\n",
      "Epoch 771\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2546 | Test loss 9304540.0000 | Test accuracy 31.25\n",
      "Epoch 772\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2742 | Test loss 10096726.0000 | Test accuracy 31.25\n",
      "Epoch 773\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3410 | Test loss 10359053.0000 | Test accuracy 31.25\n",
      "Epoch 774\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2204 | Test loss 11518091.0000 | Test accuracy 31.25\n",
      "Epoch 775\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2106 | Test loss 12354642.0000 | Test accuracy 31.25\n",
      "Epoch 776\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2893 | Test loss 11618154.0000 | Test accuracy 31.25\n",
      "Epoch 777\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2606 | Test loss 10066388.0000 | Test accuracy 31.25\n",
      "Epoch 778\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2528 | Test loss 10991895.0000 | Test accuracy 31.25\n",
      "Epoch 779\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2458 | Test loss 11008678.0000 | Test accuracy 31.25\n",
      "Epoch 780\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3402 | Test loss 11258181.0000 | Test accuracy 31.25\n",
      "Epoch 781\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3003 | Test loss 10889106.0000 | Test accuracy 31.25\n",
      "Epoch 782\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3157 | Test loss 10521530.0000 | Test accuracy 31.25\n",
      "Epoch 783\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2552 | Test loss 10763928.0000 | Test accuracy 31.25\n",
      "Epoch 784\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2981 | Test loss 10693942.0000 | Test accuracy 31.25\n",
      "Epoch 785\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2093 | Test loss 10726068.0000 | Test accuracy 31.25\n",
      "Epoch 786\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2020 | Test loss 10816072.0000 | Test accuracy 31.25\n",
      "Epoch 787\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2523 | Test loss 10589782.0000 | Test accuracy 31.25\n",
      "Epoch 788\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3415 | Test loss 11066328.0000 | Test accuracy 31.25\n",
      "Epoch 789\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3633 | Test loss 10385996.0000 | Test accuracy 31.25\n",
      "Epoch 790\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3373 | Test loss 10199740.0000 | Test accuracy 31.25\n",
      "Epoch 791\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2380 | Test loss 10897255.0000 | Test accuracy 31.25\n",
      "Epoch 792\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3751 | Test loss 11092204.0000 | Test accuracy 31.25\n",
      "Epoch 793\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3033 | Test loss 11908824.0000 | Test accuracy 31.25\n",
      "Epoch 794\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1861 | Test loss 12491248.0000 | Test accuracy 31.25\n",
      "Epoch 795\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2738 | Test loss 11894608.0000 | Test accuracy 31.25\n",
      "Epoch 796\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2554 | Test loss 11605763.0000 | Test accuracy 31.25\n",
      "Epoch 797\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2238 | Test loss 10863378.0000 | Test accuracy 31.25\n",
      "Epoch 798\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2706 | Test loss 10904100.0000 | Test accuracy 31.25\n",
      "Epoch 799\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1982 | Test loss 11682122.0000 | Test accuracy 31.25\n",
      "Epoch 800\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2342 | Test loss 11925094.0000 | Test accuracy 31.25\n",
      "Epoch 801\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2116 | Test loss 12116164.0000 | Test accuracy 31.25\n",
      "Epoch 802\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2647 | Test loss 11270074.0000 | Test accuracy 31.25\n",
      "Epoch 803\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3656 | Test loss 11807692.0000 | Test accuracy 31.25\n",
      "Epoch 804\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2588 | Test loss 10234054.0000 | Test accuracy 31.25\n",
      "Epoch 805\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1922 | Test loss 10762374.0000 | Test accuracy 31.25\n",
      "Epoch 806\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2591 | Test loss 11198772.0000 | Test accuracy 31.25\n",
      "Epoch 807\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1892 | Test loss 10767348.0000 | Test accuracy 31.25\n",
      "Epoch 808\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2820 | Test loss 10297712.0000 | Test accuracy 31.25\n",
      "Epoch 809\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3025 | Test loss 10483406.0000 | Test accuracy 31.25\n",
      "Epoch 810\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3756 | Test loss 10858270.0000 | Test accuracy 31.25\n",
      "Epoch 811\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2853 | Test loss 11454488.0000 | Test accuracy 31.25\n",
      "Epoch 812\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2240 | Test loss 11858985.0000 | Test accuracy 31.25\n",
      "Epoch 813\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2497 | Test loss 11415774.0000 | Test accuracy 31.25\n",
      "Epoch 814\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2863 | Test loss 10846177.0000 | Test accuracy 31.25\n",
      "Epoch 815\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3951 | Test loss 11944649.0000 | Test accuracy 31.25\n",
      "Epoch 816\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2282 | Test loss 12742029.0000 | Test accuracy 31.25\n",
      "Epoch 817\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2095 | Test loss 12096440.0000 | Test accuracy 31.25\n",
      "Epoch 818\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3438 | Test loss 12093236.0000 | Test accuracy 31.25\n",
      "Epoch 819\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3646 | Test loss 11798846.0000 | Test accuracy 31.25\n",
      "Epoch 820\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3549 | Test loss 10689134.0000 | Test accuracy 31.25\n",
      "Epoch 821\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2394 | Test loss 11647462.0000 | Test accuracy 31.25\n",
      "Epoch 822\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2952 | Test loss 11589030.0000 | Test accuracy 31.25\n",
      "Epoch 823\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4690 | Test loss 11025420.0000 | Test accuracy 31.25\n",
      "Epoch 824\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3330 | Test loss 10786145.0000 | Test accuracy 31.25\n",
      "Epoch 825\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2670 | Test loss 11133940.0000 | Test accuracy 31.25\n",
      "Epoch 826\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3052 | Test loss 10844732.0000 | Test accuracy 31.25\n",
      "Epoch 827\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2668 | Test loss 11702560.0000 | Test accuracy 31.25\n",
      "Epoch 828\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1941 | Test loss 12810994.0000 | Test accuracy 31.25\n",
      "Epoch 829\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2397 | Test loss 12065570.0000 | Test accuracy 31.25\n",
      "Epoch 830\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2847 | Test loss 12496640.0000 | Test accuracy 31.25\n",
      "Epoch 831\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.0972 | Test loss 12512558.0000 | Test accuracy 31.25\n",
      "Epoch 832\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2241 | Test loss 11687489.0000 | Test accuracy 31.25\n",
      "Epoch 833\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2741 | Test loss 11637180.0000 | Test accuracy 31.25\n",
      "Epoch 834\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2804 | Test loss 12919096.0000 | Test accuracy 31.25\n",
      "Epoch 835\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2904 | Test loss 10999373.0000 | Test accuracy 31.25\n",
      "Epoch 836\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2627 | Test loss 11306044.0000 | Test accuracy 31.25\n",
      "Epoch 837\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3747 | Test loss 10923282.0000 | Test accuracy 31.25\n",
      "Epoch 838\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.0745 | Test loss 11514434.0000 | Test accuracy 31.25\n",
      "Epoch 839\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1578 | Test loss 12193155.0000 | Test accuracy 31.25\n",
      "Epoch 840\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2676 | Test loss 12316718.0000 | Test accuracy 31.25\n",
      "Epoch 841\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3065 | Test loss 11062406.0000 | Test accuracy 31.25\n",
      "Epoch 842\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1794 | Test loss 12040524.0000 | Test accuracy 31.25\n",
      "Epoch 843\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2135 | Test loss 12107198.0000 | Test accuracy 31.25\n",
      "Epoch 844\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1340 | Test loss 11880507.0000 | Test accuracy 31.25\n",
      "Epoch 845\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2912 | Test loss 9880753.0000 | Test accuracy 31.25\n",
      "Epoch 846\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3202 | Test loss 11917845.0000 | Test accuracy 31.25\n",
      "Epoch 847\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3676 | Test loss 10895272.0000 | Test accuracy 31.25\n",
      "Epoch 848\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3625 | Test loss 10451036.0000 | Test accuracy 31.25\n",
      "Epoch 849\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1848 | Test loss 11359127.0000 | Test accuracy 31.25\n",
      "Epoch 850\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3077 | Test loss 12487672.0000 | Test accuracy 31.25\n",
      "Epoch 851\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3475 | Test loss 12052796.0000 | Test accuracy 31.25\n",
      "Epoch 852\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1765 | Test loss 11105520.0000 | Test accuracy 31.25\n",
      "Epoch 853\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2447 | Test loss 10365298.0000 | Test accuracy 31.25\n",
      "Epoch 854\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3078 | Test loss 10235494.0000 | Test accuracy 31.25\n",
      "Epoch 855\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2682 | Test loss 10691716.0000 | Test accuracy 31.25\n",
      "Epoch 856\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1980 | Test loss 9952936.0000 | Test accuracy 31.25\n",
      "Epoch 857\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2588 | Test loss 10140234.0000 | Test accuracy 31.25\n",
      "Epoch 858\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1634 | Test loss 11190796.0000 | Test accuracy 31.25\n",
      "Epoch 859\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1604 | Test loss 11452958.0000 | Test accuracy 31.25\n",
      "Epoch 860\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1722 | Test loss 11020542.0000 | Test accuracy 31.25\n",
      "Epoch 861\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2078 | Test loss 12370254.0000 | Test accuracy 31.25\n",
      "Epoch 862\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2554 | Test loss 11933034.0000 | Test accuracy 31.25\n",
      "Epoch 863\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1394 | Test loss 12035329.0000 | Test accuracy 31.25\n",
      "Epoch 864\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1894 | Test loss 11653514.0000 | Test accuracy 31.25\n",
      "Epoch 865\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2629 | Test loss 11395742.0000 | Test accuracy 31.25\n",
      "Epoch 866\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3262 | Test loss 11433996.0000 | Test accuracy 31.25\n",
      "Epoch 867\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2038 | Test loss 13216061.0000 | Test accuracy 31.25\n",
      "Epoch 868\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2986 | Test loss 12223704.0000 | Test accuracy 31.25\n",
      "Epoch 869\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1609 | Test loss 12441918.0000 | Test accuracy 31.25\n",
      "Epoch 870\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2153 | Test loss 13428294.0000 | Test accuracy 31.25\n",
      "Epoch 871\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2399 | Test loss 11709014.0000 | Test accuracy 31.25\n",
      "Epoch 872\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2292 | Test loss 10740290.0000 | Test accuracy 31.25\n",
      "Epoch 873\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3327 | Test loss 11040513.0000 | Test accuracy 31.25\n",
      "Epoch 874\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2379 | Test loss 12498788.0000 | Test accuracy 31.25\n",
      "Epoch 875\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3738 | Test loss 10928806.0000 | Test accuracy 31.25\n",
      "Epoch 876\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2323 | Test loss 11285759.0000 | Test accuracy 31.25\n",
      "Epoch 877\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1883 | Test loss 13017512.0000 | Test accuracy 31.25\n",
      "Epoch 878\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2261 | Test loss 12571850.0000 | Test accuracy 31.25\n",
      "Epoch 879\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1403 | Test loss 11875956.0000 | Test accuracy 31.25\n",
      "Epoch 880\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4366 | Test loss 11889651.0000 | Test accuracy 31.25\n",
      "Epoch 881\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2633 | Test loss 11446094.0000 | Test accuracy 31.25\n",
      "Epoch 882\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2310 | Test loss 11961071.0000 | Test accuracy 31.25\n",
      "Epoch 883\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1023 | Test loss 12525204.0000 | Test accuracy 31.25\n",
      "Epoch 884\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1588 | Test loss 12666821.0000 | Test accuracy 31.25\n",
      "Epoch 885\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4186 | Test loss 11635836.0000 | Test accuracy 31.25\n",
      "Epoch 886\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2073 | Test loss 12260086.0000 | Test accuracy 31.25\n",
      "Epoch 887\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3342 | Test loss 11422396.0000 | Test accuracy 31.25\n",
      "Epoch 888\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1461 | Test loss 12648111.0000 | Test accuracy 31.25\n",
      "Epoch 889\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2653 | Test loss 12855214.0000 | Test accuracy 31.25\n",
      "Epoch 890\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3056 | Test loss 13280177.0000 | Test accuracy 31.25\n",
      "Epoch 891\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2486 | Test loss 13097873.0000 | Test accuracy 31.25\n",
      "Epoch 892\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2764 | Test loss 12286052.0000 | Test accuracy 31.25\n",
      "Epoch 893\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2140 | Test loss 10888512.0000 | Test accuracy 31.25\n",
      "Epoch 894\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2470 | Test loss 12153784.0000 | Test accuracy 31.25\n",
      "Epoch 895\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2650 | Test loss 11963418.0000 | Test accuracy 31.25\n",
      "Epoch 896\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2955 | Test loss 10800936.0000 | Test accuracy 31.25\n",
      "Epoch 897\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2324 | Test loss 10322864.0000 | Test accuracy 31.25\n",
      "Epoch 898\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2603 | Test loss 10318252.0000 | Test accuracy 31.25\n",
      "Epoch 899\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3207 | Test loss 11993321.0000 | Test accuracy 31.25\n",
      "Epoch 900\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1932 | Test loss 11826610.0000 | Test accuracy 31.25\n",
      "Epoch 901\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2804 | Test loss 12317777.0000 | Test accuracy 31.25\n",
      "Epoch 902\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3056 | Test loss 13691332.0000 | Test accuracy 31.25\n",
      "Epoch 903\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2580 | Test loss 12826002.0000 | Test accuracy 31.25\n",
      "Epoch 904\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2215 | Test loss 11758785.0000 | Test accuracy 31.25\n",
      "Epoch 905\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2471 | Test loss 11317495.0000 | Test accuracy 31.25\n",
      "Epoch 906\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1720 | Test loss 11627578.0000 | Test accuracy 31.25\n",
      "Epoch 907\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2069 | Test loss 12087324.0000 | Test accuracy 31.25\n",
      "Epoch 908\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2737 | Test loss 12915929.0000 | Test accuracy 31.25\n",
      "Epoch 909\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1677 | Test loss 13516152.0000 | Test accuracy 31.25\n",
      "Epoch 910\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3334 | Test loss 10846822.0000 | Test accuracy 31.25\n",
      "Epoch 911\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1687 | Test loss 11247596.0000 | Test accuracy 31.25\n",
      "Epoch 912\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2814 | Test loss 11568283.0000 | Test accuracy 31.25\n",
      "Epoch 913\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1848 | Test loss 11835647.0000 | Test accuracy 31.25\n",
      "Epoch 914\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2953 | Test loss 12696887.0000 | Test accuracy 31.25\n",
      "Epoch 915\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.4152 | Test loss 12374838.0000 | Test accuracy 31.25\n",
      "Epoch 916\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3133 | Test loss 11967187.0000 | Test accuracy 31.25\n",
      "Epoch 917\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2881 | Test loss 12842642.0000 | Test accuracy 31.25\n",
      "Epoch 918\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3517 | Test loss 11660095.0000 | Test accuracy 31.25\n",
      "Epoch 919\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2980 | Test loss 11871044.0000 | Test accuracy 31.25\n",
      "Epoch 920\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3658 | Test loss 10760134.0000 | Test accuracy 31.25\n",
      "Epoch 921\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2865 | Test loss 11707102.0000 | Test accuracy 31.25\n",
      "Epoch 922\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2386 | Test loss 12074588.0000 | Test accuracy 31.25\n",
      "Epoch 923\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2783 | Test loss 13075156.0000 | Test accuracy 31.25\n",
      "Epoch 924\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2095 | Test loss 11804809.0000 | Test accuracy 31.25\n",
      "Epoch 925\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2415 | Test loss 11287603.0000 | Test accuracy 31.25\n",
      "Epoch 926\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3379 | Test loss 13169140.0000 | Test accuracy 31.25\n",
      "Epoch 927\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2788 | Test loss 13479925.0000 | Test accuracy 31.25\n",
      "Epoch 928\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2415 | Test loss 13517730.0000 | Test accuracy 31.25\n",
      "Epoch 929\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2452 | Test loss 12681122.0000 | Test accuracy 31.25\n",
      "Epoch 930\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3110 | Test loss 14371210.0000 | Test accuracy 31.25\n",
      "Epoch 931\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3044 | Test loss 14487655.0000 | Test accuracy 31.25\n",
      "Epoch 932\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2773 | Test loss 14486495.0000 | Test accuracy 31.25\n",
      "Epoch 933\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3091 | Test loss 12376463.0000 | Test accuracy 31.25\n",
      "Epoch 934\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1811 | Test loss 13438630.0000 | Test accuracy 31.25\n",
      "Epoch 935\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2356 | Test loss 13959085.0000 | Test accuracy 31.25\n",
      "Epoch 936\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3079 | Test loss 14298464.0000 | Test accuracy 31.25\n",
      "Epoch 937\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2212 | Test loss 14054952.0000 | Test accuracy 31.25\n",
      "Epoch 938\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3455 | Test loss 13657714.0000 | Test accuracy 31.25\n",
      "Epoch 939\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2405 | Test loss 11852848.0000 | Test accuracy 31.25\n",
      "Epoch 940\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2175 | Test loss 13553006.0000 | Test accuracy 31.25\n",
      "Epoch 941\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3385 | Test loss 13110217.0000 | Test accuracy 31.25\n",
      "Epoch 942\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1985 | Test loss 13451426.0000 | Test accuracy 31.25\n",
      "Epoch 943\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3729 | Test loss 13873046.0000 | Test accuracy 31.25\n",
      "Epoch 944\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2783 | Test loss 13707838.0000 | Test accuracy 31.25\n",
      "Epoch 945\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1900 | Test loss 14053808.0000 | Test accuracy 31.25\n",
      "Epoch 946\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2015 | Test loss 15036501.0000 | Test accuracy 31.25\n",
      "Epoch 947\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2272 | Test loss 13826237.0000 | Test accuracy 31.25\n",
      "Epoch 948\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3038 | Test loss 14365633.0000 | Test accuracy 31.25\n",
      "Epoch 949\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2854 | Test loss 14302788.0000 | Test accuracy 31.25\n",
      "Epoch 950\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1226 | Test loss 13298016.0000 | Test accuracy 31.25\n",
      "Epoch 951\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3507 | Test loss 13387466.0000 | Test accuracy 31.25\n",
      "Epoch 952\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1869 | Test loss 12007379.0000 | Test accuracy 31.25\n",
      "Epoch 953\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2981 | Test loss 12137257.0000 | Test accuracy 31.25\n",
      "Epoch 954\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3124 | Test loss 13427642.0000 | Test accuracy 31.25\n",
      "Epoch 955\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1976 | Test loss 14176129.0000 | Test accuracy 31.25\n",
      "Epoch 956\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3338 | Test loss 13937620.0000 | Test accuracy 31.25\n",
      "Epoch 957\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.0893 | Test loss 13060880.0000 | Test accuracy 31.25\n",
      "Epoch 958\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2659 | Test loss 12395572.0000 | Test accuracy 31.25\n",
      "Epoch 959\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1953 | Test loss 13776246.0000 | Test accuracy 31.25\n",
      "Epoch 960\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2345 | Test loss 12490074.0000 | Test accuracy 31.25\n",
      "Epoch 961\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3510 | Test loss 12342007.0000 | Test accuracy 31.25\n",
      "Epoch 962\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2316 | Test loss 15935038.0000 | Test accuracy 31.25\n",
      "Epoch 963\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3698 | Test loss 14190072.0000 | Test accuracy 31.25\n",
      "Epoch 964\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2868 | Test loss 14047216.0000 | Test accuracy 31.25\n",
      "Epoch 965\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3548 | Test loss 14229540.0000 | Test accuracy 31.25\n",
      "Epoch 966\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2844 | Test loss 12982342.0000 | Test accuracy 31.25\n",
      "Epoch 967\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1861 | Test loss 14302154.0000 | Test accuracy 31.25\n",
      "Epoch 968\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2457 | Test loss 13210090.0000 | Test accuracy 31.25\n",
      "Epoch 969\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.0841 | Test loss 13777559.0000 | Test accuracy 31.25\n",
      "Epoch 970\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2294 | Test loss 13451529.0000 | Test accuracy 31.25\n",
      "Epoch 971\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1997 | Test loss 14303260.0000 | Test accuracy 31.25\n",
      "Epoch 972\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3227 | Test loss 13885946.0000 | Test accuracy 31.25\n",
      "Epoch 973\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3381 | Test loss 12533695.0000 | Test accuracy 31.25\n",
      "Epoch 974\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.0510 | Test loss 12656099.0000 | Test accuracy 31.25\n",
      "Epoch 975\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3665 | Test loss 12203070.0000 | Test accuracy 31.25\n",
      "Epoch 976\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3822 | Test loss 13832286.0000 | Test accuracy 31.25\n",
      "Epoch 977\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1583 | Test loss 13783859.0000 | Test accuracy 31.25\n",
      "Epoch 978\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2555 | Test loss 14455116.0000 | Test accuracy 31.25\n",
      "Epoch 979\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2258 | Test loss 13949784.0000 | Test accuracy 31.25\n",
      "Epoch 980\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3975 | Test loss 13922610.0000 | Test accuracy 31.25\n",
      "Epoch 981\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2254 | Test loss 13584974.0000 | Test accuracy 31.25\n",
      "Epoch 982\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2631 | Test loss 13432472.0000 | Test accuracy 31.25\n",
      "Epoch 983\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.1797 | Test loss 13807968.0000 | Test accuracy 31.25\n",
      "Epoch 984\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2754 | Test loss 13094900.0000 | Test accuracy 31.25\n",
      "Epoch 985\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3920 | Test loss 12162283.0000 | Test accuracy 31.25\n",
      "Epoch 986\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2028 | Test loss 13600083.0000 | Test accuracy 31.25\n",
      "Epoch 987\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3195 | Test loss 15044302.0000 | Test accuracy 31.25\n",
      "Epoch 988\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2993 | Test loss 14690252.0000 | Test accuracy 31.25\n",
      "Epoch 989\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.0945 | Test loss 13490408.0000 | Test accuracy 31.25\n",
      "Epoch 990\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3311 | Test loss 13937802.0000 | Test accuracy 31.25\n",
      "Epoch 991\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3184 | Test loss 13794350.0000 | Test accuracy 31.25\n",
      "Epoch 992\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2600 | Test loss 13730111.0000 | Test accuracy 31.25\n",
      "Epoch 993\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.3672 | Test loss 15195094.0000 | Test accuracy 31.25\n",
      "Epoch 994\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2632 | Test loss 13145127.0000 | Test accuracy 31.25\n",
      "Epoch 995\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2953 | Test loss 12641872.0000 | Test accuracy 31.25\n",
      "Epoch 996\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2156 | Test loss 13512332.0000 | Test accuracy 31.25\n",
      "Epoch 997\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2442 | Test loss 14298314.0000 | Test accuracy 31.25\n",
      "Epoch 998\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.0789 | Test loss 14766131.0000 | Test accuracy 31.25\n",
      "Epoch 999\n",
      " ----\n",
      "Looked at 0/768 samples\n",
      "Train loss 2.2245 | Test loss 13183737.0000 | Test accuracy 31.25\n"
     ]
    }
   ],
   "source": [
    "#Initialize the model \n",
    "model = pizzamodel() \n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss() \n",
    "def acc_fn(y_pred, y_true): \n",
    "    correct = torch.eq(y_pred, y_true).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc \n",
    "\n",
    "#Create the traning and testing model\n",
    "epoches = 1000\n",
    "for epoch in range(epoches): \n",
    "    print(f'Epoch {epoch}\\n ----') \n",
    "    ###TRAINING MODEL\n",
    "    model.train() \n",
    "    for batch, (train_features, train_labels) in enumerate(train_dataloader): \n",
    "        #1. Pass to the forward \n",
    "        train_pred = model(train_features)\n",
    "\n",
    "        #2. Calculate the loss \n",
    "        train_loss = loss_fn(train_pred, train_labels.type(torch.long))\n",
    "\n",
    "        #3. Backward propagation on the loss \n",
    "        train_loss.backward() \n",
    "\n",
    "        #4. Gradient descending \n",
    "        optimizer.step() \n",
    "\n",
    "        #5. Zero out the gradient \n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        if batch % 5000 == 0: \n",
    "            print(f'Looked at {batch * len(train_features)}/{len(train_dataloader.dataset)} samples')\n",
    "    \n",
    "    ###TESTING MODEL \n",
    "    model.eval()\n",
    "    with torch.inference_mode(): \n",
    "        for test_features, test_labels in test_dataloader: \n",
    "            #1. Pass to the forward \n",
    "            test_pred = model(test_features)\n",
    "\n",
    "            #2. Calculate the loss and accuracy \n",
    "            test_loss = loss_fn(test_pred, test_labels.type(torch.long))\n",
    "            test_acc = acc_fn(y_pred=torch.argmax(torch.softmax(test_pred, dim=1), dim=1),\n",
    "                              y_true=test_labels)\n",
    "            \n",
    "    print(f'Train loss {train_loss.item():.4f} | Test loss {test_loss.item():.4f} | Test accuracy {test_acc}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
